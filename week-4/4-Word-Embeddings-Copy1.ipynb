{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Exploring Semantic Spaces (Word Embeddings)\n",
    "This week, we build on last week's topic modeling techniques by taking a text corpus we have developed, specifying an underlying number of dimensions, and training a model with a neural network auto-encoder (one of Google's word2vec  algorithms) that best describes corpus words in their local linguistic contexts, and exploring their locations in the resulting space to learn about the discursive culture that produced them.\n",
    "\n",
    "This is our third document representation we have learned: First, we used word counts. Second, we used LDA topic models built around term coocurrence in the same document (i.e., a \"bag of words\"). Third, documents here are represented as densely indexed locations in dimensions, so that distances between those documents (and words) contain more information, though they require the full vector of dimension loadings (rather than just a few selected topic loadings) to describe. We will explore these spaces to understand complex, semantic relationships between words, index documents with descriptive words, identify the likelihood that a given document would have been produced by a given vector model, and explore how semantic categories can help us understand the cultures that produced them.\n",
    "\n",
    "Note that most modern natural language processing (NLP) research, at least in computer science, uses word embeddings. This is the foundation of most state-of-the-art models.\n",
    "\n",
    "Also note that the code in this Notebook can take many minutes or even hours to run. This is the case for most NLP research these days, and it's a good opportunity to start thinking about how to manage high-compute workloads, such as running code on small samples to test it, loading datafiles in [chunks](https://stackoverflow.com/a/25962187), or [multiprocessing](https://en.wikipedia.org/wiki/Multiprocessing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Pitch Your Project*</font>\n",
    "\n",
    "<font color=\"red\">In the three cells immediately following, describe **WHAT** you are planning to analyze for your final project (i.e., texts, contexts and the social game, world and actors you intend to learn about through your analysis) (<200 words), **WHY** you are going to do it (i.e., why would theory and/or the average person benefit from knowing the results of your investigation) (<200 words), and **HOW** you plan to investigate it (i.e., what are the approaches and operations you plan to perform, in sequence, to yield this insight) (<400 words).\n",
    "\n",
    "# ***What?*** \n",
    "<200 words\n",
    "\n",
    "## ***Why?***\n",
    "<200 words\n",
    "\n",
    "## ***How?***\n",
    "<400 words\n",
    "\n",
    "## <font color=\"red\">*Pitch Your Sample*</font>\n",
    "\n",
    "<font color=\"red\">In the cell immediately following, describe the rationale behind your proposed sample design for your final project. What is the social game, social work, or social actors you about whom you are seeking to make inferences? What are its virtues with respect to your research questions? What are its limitations? What are alternatives? What would be a reasonable path to \"scale up\" your sample for further analysis (i.e., high-profile publication) beyond this class? (<300 words).\n",
    "\n",
    "## ***Which (words)?***\n",
    "<300 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "import gensim#For word2vec, etc\n",
    "import requests #For downloading our datasets\n",
    "import lucem_illud #pip install -U git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas as pd #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting our corpora\n",
    "\n",
    "Instead of downloading our corpora, we have download them in advance; a subset of the [senate press releases](https://github.com/lintool/GrimmerSenatePressReleases) are in `grimmerPressReleases`. We will load them into a DataFrame, but first we need to define a function to convert directories of text files into DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDir(targetDir, category):\n",
    "    allFileNames = os.listdir(targetDir)\n",
    "    #We need to make them into usable paths and filter out hidden files\n",
    "    filePaths = [os.path.join(targetDir, fname) for fname in allFileNames if fname[0] != '.']\n",
    "\n",
    "    #The dict that will become the DataFrame\n",
    "    senDict = {\n",
    "        'category' : [category] * len(filePaths),\n",
    "        'filePath' : [],\n",
    "        'text' : [],\n",
    "    }\n",
    "\n",
    "    for fPath in filePaths:\n",
    "        with open(fPath) as f:\n",
    "# Try this line instead if you get an encoding error.\n",
    "#         with open(fPath, encoding=\"ISO-8859-1\") as f:\n",
    "            senDict['text'].append(f.read())\n",
    "            senDict['filePath'].append(fPath)\n",
    "\n",
    "    return pd.DataFrame(senDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the function in all the directories in `data/grimmerPressReleases`. If you are on Google Colab, note that rather than downloading GitHub data to your local machine and then uploading it to Drive, you can more quickly `git clone` directly to Colab, such as `!git clone https://github.com/lintool/GrimmerSenatePressReleases.git /drive/MyDrive/`, which then makes your `dataDir = 'drive/MyDrive/grimmerPressReleases/raw'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filePath</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>../data/grimmerPressReleases\\Kennedy\\01Apr2005...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>../data/grimmerPressReleases\\Kennedy\\01Dec2005...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     Washington ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>../data/grimmerPressReleases\\Kennedy\\01Feb2006...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE      Fact sheet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>../data/grimmerPressReleases\\Kennedy\\01Feb2007...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     Washington ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>../data/grimmerPressReleases\\Kennedy\\01Jun2007...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  BOSTON  MA  Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>../data/grimmerPressReleases\\Kennedy\\01Mar2007...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     Washington ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>../data/grimmerPressReleases\\Kennedy\\01May2007...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  The President ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>../data/grimmerPressReleases\\Kennedy\\01Nov2007...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  Washington  DC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>../data/grimmerPressReleases\\Kennedy\\02Aug2006...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>../data/grimmerPressReleases\\Kennedy\\02Feb2005...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE     The Preside...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                           filePath  \\\n",
       "0   Kennedy  ../data/grimmerPressReleases\\Kennedy\\01Apr2005...   \n",
       "10  Kennedy  ../data/grimmerPressReleases\\Kennedy\\01Dec2005...   \n",
       "20  Kennedy  ../data/grimmerPressReleases\\Kennedy\\01Feb2006...   \n",
       "30  Kennedy  ../data/grimmerPressReleases\\Kennedy\\01Feb2007...   \n",
       "40  Kennedy  ../data/grimmerPressReleases\\Kennedy\\01Jun2007...   \n",
       "50  Kennedy  ../data/grimmerPressReleases\\Kennedy\\01Mar2007...   \n",
       "60  Kennedy  ../data/grimmerPressReleases\\Kennedy\\01May2007...   \n",
       "70  Kennedy  ../data/grimmerPressReleases\\Kennedy\\01Nov2007...   \n",
       "80  Kennedy  ../data/grimmerPressReleases\\Kennedy\\02Aug2006...   \n",
       "90  Kennedy  ../data/grimmerPressReleases\\Kennedy\\02Feb2005...   \n",
       "\n",
       "                                                 text  \n",
       "0            FOR IMMEDIATE RELEASE   FOR IMMEDIATE...  \n",
       "10           FOR IMMEDIATE RELEASE     Washington ...  \n",
       "20           FOR IMMEDIATE RELEASE      Fact sheet...  \n",
       "30           FOR IMMEDIATE RELEASE     Washington ...  \n",
       "40           FOR IMMEDIATE RELEASE  BOSTON  MA  Se...  \n",
       "50           FOR IMMEDIATE RELEASE     Washington ...  \n",
       "60           FOR IMMEDIATE RELEASE  The President ...  \n",
       "70           FOR IMMEDIATE RELEASE  Washington  DC...  \n",
       "80           FOR IMMEDIATE RELEASE  FOR IMMEDIATE ...  \n",
       "90           FOR IMMEDIATE RELEASE     The Preside...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataDir = 'content/drive/MyDrive/grimmerPressReleases/raw'\n",
    "dataDir = '../data/grimmerPressReleases'\n",
    "\n",
    "senReleasesDF = pd.DataFrame()\n",
    "\n",
    "for senatorName in [d for d in os.listdir(dataDir) if d[0] != '.']:\n",
    "    senPath = os.path.join(dataDir, senatorName)\n",
    "    senReleasesDF = senReleasesDF.append(loadDir(senPath, senatorName), ignore_index = True)\n",
    "\n",
    "senReleasesDF[:100:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec needs to retain the sentence structure so as to capture a \"continuous bag of words (CBOW)\" and all of the skip-grams within a word window. The algorithm tries to preserve the distances induced by one of these two local structures. This is very different from clustering and LDA topic modeling which extract unordered words alone.\n",
    "\n",
    "When we normalize here, we don't use the lematized form of the word because we might lose information. Note the paramter in the normalize tokens function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en\")\n",
    "except OSError:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(word_list, extra_stop=[], model=nlp, lemma=True, MAX_LEN=1500000):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "    print('a')\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "\n",
    "    # since we're only normalizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "\n",
    "    if len(extra_stop) > 0:\n",
    "        for stopword in extra_stop:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    # we check if we want lemmas or not earlier to avoid checking every time we loop\n",
    "    if lemma:\n",
    "        print('1')\n",
    "        doc = model(word_list.lower(), disable=[\"ner\"])\n",
    "        print('2')\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.lemma_))\n",
    "    else:\n",
    "        print('3')\n",
    "        doc = model(word_list.lower(), disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"])\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.text.strip()))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesDF = senReleasesDF[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(word_list, model=nlp, MAX_LEN=1500000):\n",
    "#     print('in word_tokenize')\n",
    "    tokenized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "    # since we're only tokenizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list,disable=['tagger', 'parser', 'lemmatizer', 'ner'])\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized\n",
    "\n",
    "def sent_tokenize(word_list, model=nlp):\n",
    "#     print('in sent_tokenize')\n",
    "    doc = model(word_list,disable=['lemmatizer', 'ner'])\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "def normalizeTokens(word_list, extra_stop=[], model=nlp, lemma=True, MAX_LEN=1500000):\n",
    "    print('in normalizeTokens')\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "\n",
    "    # since we're only normalizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "\n",
    "    if len(extra_stop) > 0:\n",
    "        for stopword in extra_stop:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    # we check if we want lemmas or not earlier to avoid checking every time we loop\n",
    "    if lemma:\n",
    "        print('in if lemma')\n",
    "        print(word_list.lower())\n",
    "        print(nlp.pipe_names)\n",
    "        doc = model(word_list.lower(), disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.lemma_))\n",
    "    else:\n",
    "        print('in else')\n",
    "        doc = model(word_list.lower(), disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"])\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.text.strip()))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in normalizeTokens\n",
      "in if lemma\n",
      "for immediate release for immediate release contact melissa wagoner brent carney 202 224 2633 boston ma senator edward m kennedy announced today that worcester polytechnic institute will receive a 197 000 grant under the business and international education program of the u s department of education wpi will use the funds for a three part project on south africa to improve understanding of south africa and its economy to enable wpi students to study in south africa and to strengthen new england companies involvement in south africa wpi has received federal funds for the past three years under the business and international education program for a project focusing on namibia senator kennedy said one of the principle challenges facing our country and our economy today is globalization this grant will enable wpi to work with the south african business community and the corporate council on africa to develop closer ties between america and this critical part of the global economy i commend wpi for its commitment to increasing the business knowledge and experience of its students and faculty in the nations of africa laura capps melissa wagoner 202 224 2633\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "for immediate release for immediate release contact laura capps melissa wagoner 202 224 2633\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "the abuse of power and the cloak of secrecy from the white house continues it s bad enough that the administration stonewalled the senate by refusing to disclose documents highly relevant to the bolton nomination it s even worse for the administration to abuse the recess appointment power by making the appointment while congress is in this five week recess it s a devious maneuver that evades the constitutional requirement of senate consent and only further darkens the cloud over mr bolton s credibility at the u n\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "it s shameful and irresponsible for the administration to go forward with the appointment in the teeth of the revelation that bolton misled congress by denying he d been interviewed in the state department cia investigation of faulty pre war intelligence on iraq laura capps melissa wagoner 202 224 2633\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "for immediate release for immediate release washington d c today senator edward m kennedy sent the below letter to secretary of education margaret spellings to enforce newly enacted restrictions on the student loan program to protect students and parents against unscrupulous bank tactics\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "in february congress enacted a moratorium on a program which allowed schools to act as lenders to their students the moratorium was enacted as a result of several lenders using the program to provide inappropriate kickbacks to schools in order to entice them to use their services\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "in addition to prohibiting new schools from entering the program the law put new restrictions on the schools already in the program designed to ensure that no one loan program has an unfair competitive advantage which could result in fewer options for students and to ensure that students benefit directly from any proceeds from the program by requiring that schools use those proceeds for need based student aid since enactment of the law some financial institutions have advised schools that they can evade the new restrictions through use of a financial arrangement known as an eligible lender trustee\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "in a letter sent today senator kennedy urged secretary spellings to enforce the new law and preserve the integrity of the student aid programs by ensuring that no schools or lenders attempt to use loopholes to circumvent the restrictions on the program senator kennedy noted in the letter that the department of education has in the past issued a policy that use of such arrangements to evade the law would not be tolerated\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "the letter is included below a pdf is available upon request august 1 2006 the honorable margaret spellings secretary department of education 400 maryland avenue sw washington d c 20202 dear secretary spellings\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "as you know in february congress established a moratorium on the entry of new schools into the school as lender program and enacted a number of specific obligations on institutions already in the program since the new restrictions were enacted it has come to my attention that a growing number of schools are being advised by financial institutions that restrictions on the program can be circumvented through the use of an eligible lender trustee i urge you to ensure that this loophole is not used to circumvent the law as you know several lenders have used the school as lender program to provide inappropriate kickbacks to participating schools in order to entice them to use their services schools are technically the lender for a short period of time but a private lender ultimately holds the loan and gives the school a percentage of future profits the new restrictions in the program were designed to ensure that no one loan program has an unfair competitive advantage resulting in fewer options for students these limitations also ensure that students benefit directly from any profits stemming from a schools participation in the lending process by requiring schools to use the proceeds for need based student aid prior to the moratorium on the program schools in states with laws banning them from participating in the lending process were able to participate in the school as lender program through the creation of an eligible lender trust as long as they abided by the statutory restrictions on the program creation of this type of arrangement is no longer relevant since congress has banned additional schools from becoming lenders nevertheless members of the lending community are encouraging schools to use an eligible lender trust solely to circumvent the new federal restrictions the department of education recognized the potential for abuse and issued a dear colleague letter in 1995 to clarify that arrangements intended to evade the restrictions on the school as lender program would not be tolerated i urge you to continue this policy of preventing lenders from using loopholes to violate the law millions of students and families across the nation are struggling to afford college and they put their trust in the federal government to ensure the integrity of the financial aid process aggressive enforcement of the restrictions on the school as lender program is critical to protecting their interests\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "thank you in advance for your assistance on this very important issue with respect and appreciation sincerely edward m kennedy laura capps melissa wagoner 202 224 2633\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "for immediate release for immediate release washington d c below is senator kennedys statement on at the nomination hearing of dr von eschenbach to be commissioner of the food and drug administration mr chairman thank you for holding todays hearing on the nominations of dr andrew von eschenbach to be commissioner of food and drugs and paul decamp to serve as administrator of the wage and hour division at the department of labor these are two critical positions worthy of serious consideration and i appreciate this opportunity hear from them and learn more about them i ve always been a strong supporter of the national cancer institute and i admire dr von eschenbachs leadership there especially on initiatives such as genomics and nanotechnology\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "as a survivor of cancer himself and as a physician for patients with cancer he has brought an important patient centered perspective to the institute and hell bring it to the food and drug administration as well\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "in the controversies over antidepressants and suicidal behavior in children the withdrawal of vioxx the agencys refusal to approve the sale of plan b over the counter we ve seen the fda struggling with difficult scientific questions inadequate resources and authority and unfair pressures to ignore science fda needs a strong commissioner to deal with these and other issuesto refocus the agency and enable it to make decisions based solely on science developed after an open unencumbered scientific debate not on ideology or political expediency the pending decision on plan b is a test case of fdas integrity yesterday dr von eschenbach announced that fda would not pursue the rulemaking that the administration had previously claimed was needed to respond to this application in favor of a more informal negotiation with the manufacturer\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "if this step leads to a swift and clear decision i applaud it\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "but we must make certain that the administration does not use it as yet another delaying tactic serious concerns have been raised about the degree to which political pressures influenced fdas actions on plan b\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "and i urge dr von eschenbach to use the upcoming negotiations to begin to allay those concerns not raise them anew sadly plan b is not the only example in which this administration has pressured fda to value political considerations over the statutes under which it operates and the science before it a recent survey by the union of concerned scientists presents serious evidence of problems at the agency a majority of the agencys scientists who responded disagreed or strongly disagreed that fdas leadership consistently stands behind scientific staff or managers who propose science supported decisions even though they may be politically controversial fda has long been regarded as the gold standard in its regulatory work but that will continue to be the case only if it makes independent science based decisions in both fact and appearance under dr von eschenbachs leadership we expect fda to make decisions solely on the basis of science and in the best interest of public health i hope that he will assure us that fda management will no longer reject the view of agency scientists or actively discourage them from voicing their concerns next year will be an important year for the fdafour reauthorizations will come before this committee and we will need the agencys assistance to enact them senator enzi and i are preparing to introduce a drug safety bill which will require the agency and companies to develop a strategy to consider the post approval safety of a drug i hope we will have dr von eschenbachs support for that effort and the support of all at the agency laura capps melissa wagoner 202 224 2633\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "for immediate release for immediate release contact laura capps melissa wagoner august 1 2006 202 224 2633 gao report on workers lacking critical protections included washington d c today senator edward m kennedy spoke out against the nomination of paul decamp as administrator of the wage and hour division at the department of labor and released a new gao report finding millions of workers are currently lacking key workplace protections mr decamp an attorney who has represented wal mart has a long and troubling record on workers rights decamp has proposed overtime rollbacks for millions of workers and has expressed his view that most claims for unpaid overtime are spurious he now serves in a temporary position at the department of labor where he has been heavily involved in the departments disastrous failure to protect workers from abuses of wage laws and overtime protections in the gulf coast region choosing mr decamp to enforce laws he does nt believe in and protections he does nt support seems to be a flagrant example of the fox guarding the henhouse the safeguards of our nations wage and hour laws protect employees basic rights senator kennedy said we need strong leadership at the wage and hour division to make these rights a reality for all americans on decamps work related to the gulf coast kennedy added he has been deeply involved in the departments abysmal failure to protect the rights of workers in the gulf coast senator kennedy said an alarming number of those men and women are not being paid at all for their work or are not being paid the wages required by law or are being forced to work overtime without overtime pay yet the department of labor is awol from the region and has made it almost impossible for workers to file claims when their rights are violated if confirmed decamp would be responsible for enforcing critical worker protections such as minimum wage overtime pay and the child labor protections of the fair labor standards act senator kennedy strongly believes that he is the wrong person for the job senator kennedy also released a gao report revealing a significant portion of the american workforce lacks critical workplace protections because they are classified as contingent workers with fewer rights currently 42 6 million workers are classified as contingent workers where they are less likely to have health insurance or pension benefits and are often not covered by key workplace protection laws\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "in response to the gao report senator kennedy said i am deeply disturbed by the findings of this report which reveal the bush administrations cavalier approach to protecting workers rights the misclassification of employees as independent contractors is a serious problem that cheats workers out of hard earned pay and threatens their health and safety the department of labor must do much more to educate workers about their rights and it must undertake vigilant enforcement efforts to detect and remedy worker misclassification a summary of the report follows senator kennedys statement below is his prepared statement statement by senator edward m kennedy on nomination of paul decamp as prepared for delivery\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "today we also have the opportunity to consider another important nomination that of paul decamp to be administrator of the wage and hour division at the department of labor this vital position is charged with enforcing many of our most critical worker protections including the minimum wage overtime pay the child labor protections of the fair labor standards act the leave requirements of the family and medical leave act and the prevailing wage requirements of the davis bacon act and the service contract act the actions of the wage and hour administrator affect the lives of every worker in america he ensures that men and women who work overtime will be able to rely on overtime pay to make ends meet he protects working teenagers from being forced to use dangerous equipment that could threaten their health or safety he ensures that parents who need to care for sick children can meet their family needs and still return to their jobs he defends vulnerable employees such as migrant workers and day laborers when they are exploited by unscrupulous employers unfortunately the bush administration has shown a troubling lack of commitment to protecting workers rights in these areas they have adopted regulations that could deny overtime rights from as many as 6 million workers they ve made sweetheart deals that let repeat offenders like wal mart off the hook for violating child labor laws year after year they ve refused to raise the minimum wage for hardworking people living below the poverty line they ve failed to protect the rights of the hard working men and women rebuilding the gulf coast the nomination of mr decamp raises troubling questions his record clearly demonstrates that he does not have the commitment to workers rights that is necessary to fulfill the goals of these important laws his extensive record of publications shows that he does not support the goals of the statutes he will be responsible for enforcing\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "he s advocated changes in current law to drastically reduce the number of employees entitled to overtime pay and he s suggested that those who work overtime but are denied overtime pay by their employers do not deserve the remedies these laws provide his record in private practice is equally disturbing he has a long history of representing employers with strong anti labor agendas his ties to wal mart one of the industrys most notorious offending employers calls into question whether he can carry out his responsibilities with the perspective necessary to protect the rights of workers since joining the department of labor in an advisory capacity last year he has been deeply involved in the departments abysmal failure to protect the rights of workers in the gulf coast an alarming number of those men and women are not being paid at all for their work or are not being paid the wages required by law or are being forced to work overtime without overtime pay yet the department of labor is awol from the region and has made it almost impossible for workers to file claims when their rights are violated choosing mr decamp to enforce laws he does nt believe in and protections he does nt support\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "seems to be a flagrant example of the fox guarding the henhouse the safeguards of our nations wage and hour laws protect employees basic rights hardworking men and women have a right to a fair days pay for a fair days work they have a right to overtime pay if the law provides it they have a right to keep their jobs if they become ill or have to care for a sick family member we need strong leadership at the wage and hour division to make these rights a reality for all americans thank you mr chairman for this hearing and i look forward to the testimony of our nominees senator kennedy releases new gao report contingent workers lose critical workplace protections a government accountability office report released today by senator edward m kennedy reveals that a significant portion of the american workforce lacks critical workplace protections because they are classified as contingent workers and do not receive the same rights as other employees the report entitled employment arrangements improved outreach could help ensure proper worker classification finds that about 42 6 million u s workers are currently employed in contingent work arrangements where they do not receive the benefits of the traditional employment relationship these workers are far less likely to have health insurance or pension benefits and they are often not covered by key workplace protection laws like antidiscrimination laws overtime and minimum wage laws or workplace health and safety laws only 13 percent of contingent workers receive healthcare through their workplace compared to 73 percent of regular full time workers only 38 percent of contingent workers receive pensions compared to 76 percent of other full time workers the report further finds that too many employers are intentionally misclassifying their workers as independent contractors as a result of such misclassification an employee who works 40 hours a week 52 weeks a year for the same employer follows that employers instructions and receives their paycheck from that employer can be denied basic benefits and basic rights like the right to family and medical leave or the right to be free from discrimination because their employer is engaging in legal trickery a class action lawsuit is currently pending against wal mart stores alleging that the retail giant misclassified thousands of janitorial workers as independent contractors and refused to pay them minimum wages or overtime pay to which they were lawfully entitled wal mart is alleging that these workers are independent contractors despite the fact that the store had the power to hire and fire them controlled their wages hours and working conditions and directed them in the day to day tasks of their work such misclassification cheats these workers out of protections they lawfully deserve and have rightfully earned while cheating the government out of needed tax revenues\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "according to the most recent data from the internal revenue service 15 percent of employers have misclassified employees as independent contractors these misclassifications result in a loss to the federal government of about 2 7 billion per year in tax revenue unfortunately the department of labors efforts to detect and prevent these abuses are seriously deficient and workers are left without the assistance they need to protect their rights the department is not undertaking any effort to detect employee misclassification and while the department relies on workers to report violations of the law it is not even attempting to educate workers about their rights the departments standard poster informing workers about their rights in the workplace does not contain any information on employee misclassification or inform employees how to register a complaint that they have been misclassified while the departments procedures require officials to share information with other federal and state agencies when they uncover a misclassification problem that may violate other laws such as tax laws or unemployment insurance laws the gao found that the department does not consistently follow this directive in response to the gao report senator kennedy said i am deeply disturbed by the findings of this report which reveal the bush administrations cavalier approach to protecting workers rights the misclassification of employees as independent contractors is a serious problem that cheats workers out of hard earned pay and threatens their health and safety the department of labor must do much more to educate workers about their rights and it must undertake vigilant enforcement efforts to detect and remedy worker misclassification\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "for immediate release for immediate release washington dc as the senate prepares to vote this week on an estate tax minimum wage package senator kennedy reiterated his opposition to the measure noting that the republican plan includes a pay cut for workers who depend on tips for a living the bill would change the minimum wage for tipped workers seven states alaska california minnesota nevada oregon and washington requiring them to be paid only the federal minimum wage not the higher state minimum wage until the state enacts a law with a tip penalty the restaurant industry fought for this provision to be included in the republican package\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "the republican bill would boost the bottom line for americas restaurants while taking money away from waiters and waitresses parking attendants bellhops and other hardworking americans who depend on tips to support themselves and their families senator kennedy said instead of denying millions of workers minimum wage protection we should raise the wage and expand the protection the people who work in our restaurants carry our bags and clean our hotel rooms are work hard for a living and they deserve a well earned raise i m confident this cynical ploy will be rejected by the senate below is a fact sheet on the tip penalty under current federal law restaurant owners can pay their waiters and waitresses as little as only 2 13 an hour and the rest of their compensation is supposed to come from tips the same is true for hotel maids parking attendants bartenders all workers who rely on tips to make a living congress has increased this cash wage only 12 cents in the last 25 years federal labor and employment law sets a minimum floor but states are free to guarantee higher wage for tipped workers in fact the fair labor standards act encourages states to enact laws that are more protective for workers than the federal law seven states alaska california minnesota montana nevada oregon and washington do not allow a tip penalty they guarantee that tipped workers get the full minimum wage plus any tips they receive but the republican bill would take power away from the states by nullifying these state laws providing stronger wage protections for tipped employees than the federal standard in fact the bill would change the minimum wage for tipped workers in these seven states requiring them to be paid only the federal minimum wage not the higher state minimum wage until the state enacts a law with a tip penalty washington and oregon have passed ballot initiatives to provide stronger wage protections to tipped employees than the federal protections this amendment overturns the will of the voters in those states it seems that republicans support states rights except when it comes to fair wages for workers then republicans support what s good for the restaurant industry laura capps melissa wagoner 202 224 2633\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "for immediate release for immediate release mr chairman i m concerned that the committee is proceeding with undue haste on the nomination of mr keisler there are important unresolved matters that we should consider before we reach a decision first is the issue we raised in the letter we sent you last week\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "urging the committee to examine the need to fill the 11th or 12th judgeships on the d c circuit republican members of this committee strongly opposed attempts by the clinton administration to fill the 11th seat and they were successful in blocking well qualified nominees\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "they argued that the court did not have enough cases to justify that number of judges since then the number of written opinions issued by the court has declined by 17 the number of cases resolved on the merits per judge is down 21 and the number of cases filed per judge is down 10 we should consider these caseload declines carefully before we fill the current vacancy american taxpayers deserve no less in addition we have had very little time to consider the record of mr keisler he was nominated only a month ago and the aba did not complete its evaluation of him until yesterday as we all know the d c circuit is second in importance only to the supreme court we should proceed with particular care in confirming judges to that court\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "in fact among the last seven d c circuit nominees the shortest period from nomination to hearing was 71 days we have barely had 30 days for mr keisler this rush has left very little time to study and in some instances even to assemble his record we know that he has served in high government positions and has had a successful private practice and has received a well qualified rating by the aba but we have had little real opportunity to examine his record we know that he worked in the reagan white house but we know virtually nothing about what he worked on there we have not had the opportunity to obtain records from that period of his career\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "we know that he was a founder and longtime high ranking officer in the federalist society and that he was upset that judge bork was not confirmed to the supreme court indeed he dismissed criticism of judge borks record stating its just a bunch of hot air i think bork is in the mainstream and he is reported to have said it was extremely frustrating to see ideas that had previously been considered part of a reasonable debate excommunicated and defined as extreme by the senate as one who sat on this committee when judge bork was considered i disagree strongly with those views i therefore hope that we will have an opportunity to look carefully into all of these issues before we proceed to vote the merits of this nomination laura capps melissa wagoner 202 224 2633\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "for immediate release washington dc today senator edward m kennedy chairman of the senate health education labor and pensions committee released the following statement in response to the passage by the help committee of the family smoking prevention and tobacco control act today we put children back on the national agenda the bipartisan legislation will save millions of lives and save others from a lifetime of addiction and certain death the tobacco industry has been allowed to mislead consumers to make false health claims to conceal the lethal contents of their products to make their products even more addictive and worst of all to seduce generations of children into a lifetime of addiction and early death enacting this bill this year is the right thing to do for americas children they are depending on us by passing this legislation we can help them live longer healthier lives laura capps melissa wagoner 202 224 2633\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in normalizeTokens\n",
      "in if lemma\n",
      "for immediate release rachel racusen miller 202 226 0853christina mulka durbin 202 228 5643alec gerlach kildee 202 225 3611 new gao report underscores administrations failures to safeguard federal student loan program education department must increase oversight of student lenderswashington d c the u s department of education has failed to safeguard the nations federal student loan programs and should immediately increase its oversight of lenders and schools and fully enforce the law government investigators concluded in a new report released today by leading democratic lawmakers todays report comes after months of congressional and state investigations have uncovered unethical financial relationships among lenders school financial aid officers and public officials responsible for overseeing the federal student loan program in the departments office of federal student aid under current law lenders participating in the federal loan program are prohibited from using inducements or gifts to curry favor with colleges or universities the report which was prepared by the government accountability office at the request of reps george miller d ca and dale e kildee d mi and senators edward m kennedy d ma and dick durbin d il found that the department does not have a sufficient oversight program in place to identify and address questionable lender behavior such as inducements\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "in addition the report found that despite repeated requests from lenders for the department to provide direction on inducements the department had not updated its inducement guidelines in nearly 20 years and in some cases did not respond to lenders inquiries at all in 2003 the departments inspector general also urged the department to issue a dear colleague letter to lenders and schools on its regulations regarding inducements and gifts the report also determined that the department had a poor system for dealing with complaints of improper lender behavior for example out of 26 documented complaints received by the department between 2001 and 2006 only two complaints prompted action by the department and 14 complaints were left unresolved in addition the report found that the department attempted to use its sanctioning authority against lenders accused of improper inducements only twice over the past twenty years this report again underscores that the department of education completely defaulted on its responsibilities to protect the nations student loan programs said miller the chairman of the house education and labor committee there is simply no excuse for this administration ignoring repeated warnings about potential lender abuses both from independent agencies and even from lenders themselves earlier this year i called on the secretary to take emergency actions to hold lenders and schools accountable and enforce the law today i again urge her to start doing the job she was entrusted with by immediately implementing this reports recommendations students and families should be deeply concerned that the department of education failed to enforce the laws designed to protect them from unscrupulous lender tactics for so long said kennedy the chairman of the senate committee on health education labor and pensions the higher education bill passed unanimously by the senate last week provides even more protections for borrowers but they ll only be effective with proper follow through by the department i hope secretary spellings will do everything in her power to build on her recent efforts to increase enforcement of the current laws against improper lender inducements and carefully consider the recommendations in this report for those of us in the congress who have been working to protect students from exploitation todays gao report has confirmed our fears said durbin lenders continue to take advantage of students entering college and the department of education is not doing enough to prevent it parents and students have placed their trust in a department that is not living up to its responsibility if secretary spellings ignores this grim report more and more students will be saddled with avoidable debt for years to come the departments failure to conduct even the most basic oversight is a great disservice to hard working students and their families said kildee the chairman of the subcommittee on early childhood elementary and secondary education i hope that the secretary finally will take this opportunity to rectify the results of this prior inaction in may u s education secretary margaret spellings appeared before the house education and labor committee to testify about the departments role in conducting oversight of the federal student loan programs and the reading first program another education program that has been marred by conflicts of interest in april new york attorney general andrew cuomo http\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "www house gov apps list speech edlabor_dem rel042507 html http www house gov apps list speech edlabor_dem rel042507 html told the committee that the bush administration had been asleep at the switch when it came to providing oversight over the student loan programs over the past several months miller and kennedy have been conducting investigations into the student loan industry both chambers of congress have passed legislation that would clean up the relationships between lenders and schools in february miller and kennedy introduced the bipartisan student loan sunshine act which the house overwhelmingly http www house gov apps list speech edlabor_dem rel050907a html http www house gov apps list speech edlabor_dem rel050907a html passed in may key provisions of the bill were incorporated into the senates reauthorization of the higher education act the higher education amendments of 2007 to see a pdf copy of the gao report click here http\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "edlabor house gov documentation 20070801gaoffelp pdf http edlabor house gov documentation 20070801gaoffelp pdf\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "melissa wagoner 202 224 2633\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "in normalizeTokens\n",
      "in if lemma\n",
      "for immediate release washington dc today senator edward m kennedy chairman of the senate health education labor and pensions committee released the following statement in response to the passage by chairman dodd and the banking committee of lender ethics reform legislation the bill passed by chairman dodd and the banking committee today is an important complement to the bipartisan lender ethics reforms the senate unanimously approved last week in our higher education reauthorization bill at a time when the private student loan market is growing by 27 percent a year its critical that our reforms cover both private loans and loans made through the federal governments student loan program by applying the gift ban and consumer disclosure provisions from our bill to private loans and by extending additional banking law protections to students who take out such loans chairman dodds legislation will protect students and families from exploitation and ensure that they get the best deal on their private loans laura capps melissa wagoner 202 224 2633\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filePath</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>../data/grimmerPressReleases\\Kennedy\\01Apr2005...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE   FOR IMMEDIATE...</td>\n",
       "      <td>[[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...</td>\n",
       "      <td>[[immediate, release, immediate, release, cont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                           filePath  \\\n",
       "0  Kennedy  ../data/grimmerPressReleases\\Kennedy\\01Apr2005...   \n",
       "\n",
       "                                                text  \\\n",
       "0           FOR IMMEDIATE RELEASE   FOR IMMEDIATE...   \n",
       "\n",
       "                                     tokenized_sents  \\\n",
       "0  [[FOR, IMMEDIATE, RELEASE, FOR, IMMEDIATE, REL...   \n",
       "\n",
       "                                    normalized_sents  \n",
       "0  [[immediate, release, immediate, release, cont...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply our functions, notice each row is a list of lists now\n",
    "senReleasesDF['tokenized_sents'] = senReleasesDF['text'].apply(lambda x: [word_tokenize(s) for s in sent_tokenize(x)])\n",
    "#senReleasesDF['normalized_sents'] = senReleasesDF['tokenized_sents'].apply(lambda x: [lucem_illud.normalizeTokens(s, lemma=False) for s in x])\n",
    "senReleasesDF['normalized_sents'] = senReleasesDF['tokenized_sents'].apply(lambda x: [normalizeTokens(s) for s in x])\n",
    "\n",
    "senReleasesDF[:100:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "We will be using the gensim implementation of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec). Note that newer versions of gensim have different syntax, but if you're using a newer version, you should see informative error messages (e.g., replace `senReleasesW2V['president'][:10]` with `senReleasesW2V['president'].wv[:10]`). The following lines should work out-of-the-box on Google Colab.\n",
    "\n",
    "To load our data our data we give all the sentences to the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sg=0 argument indicates that we don't want to use \"skipgram\" but instead \"CBOW\"\n",
    "senReleasesW2V = gensim.models.word2vec.Word2Vec(senReleasesDF['normalized_sents'].sum(), sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the word2vec object, each word has a vector. To access the vector directly, use the square braces (`__getitem__`) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesW2V['president'][:10] #Shortening because it's very large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want the full matrix, `syn0` stores all the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesW2V.wv.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, `index2word` lets you translate from the matrix to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesW2V.wv.index2word[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a few things that come from the word vectors. The first is to find similar vectors (cosine similarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesW2V.most_similar('president')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesW2V.most_similar('war')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which word least matches the others within a word set (cosine similarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesW2V.doesnt_match(['administration', 'administrations', 'presidents', 'president', 'washington'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which word best matches the result of a semantic *equation* (here, we seek the words whose vectors best fit the missing entry from the equation: **X + Y - Z = _**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesW2V.most_similar(positive=['clinton', 'republican'], negative = ['democrat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that **Clinton + Republican - Democrat = Bush**. In other words, in this dataset, **Clinton** is to **Democrat** as **Bush** is to **Republican**. Whoah!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the vectors for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senReleasesW2V.save(\"senpressreleasesWORD2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use dimension reduction to visulize the vectors. We will start by selecting a subset we want to plot. Let's look at the top words from the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numWords = 50\n",
    "targetWords = senReleasesW2V.wv.index2word[:numWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then extract their vectors and create our own smaller matrix that preserved the distances from the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsSubMatrix = []\n",
    "for word in targetWords:\n",
    "    wordsSubMatrix.append(senReleasesW2V[word])\n",
    "wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "wordsSubMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use PCA to reduce the dimesions (e.g., to 50), and T-SNE to project them down to the two we will visualize. We note that this is nondeterministic process, and so you can repeat and achieve alternative projectsions/visualizations of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaWords = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_data = pcaWords.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWords = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can plot the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWords[:, 0], tsneWords[:, 1], alpha = 0)#Making the points invisible \n",
    "for i, word in enumerate(targetWords):\n",
    "    ax.annotate(word, (tsneWords[:, 0][i],tsneWords[:, 1][i]), size =  20 * (numWords - i) / numWords)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My visualization above puts ``said`` next to ``congress`` and ``bill`` near ``act``. ``health`` is beside ``care`` and ``national`` abuts ``security``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8SGFeWSehSQ"
   },
   "source": [
    "# Chinese examples\n",
    "The following datafile is [available on Canvas](https://canvas.uchicago.edu/courses/39937/files/6674623/download). The Chinese word2vec model we use in this assignment was made with an old version of Gensim. If you have a version of '4.0.0' or later, you can run code like `!pip install gensim==3.8.3` (the last v3 of Gensim before v4) and restart your notebook to load that file. However, it's usually best to use the most recent stable versions of Python packages, so you can skip this section and go straight to the FastText section if you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "gpZ6KZrOyKYS",
    "outputId": "a7836df2-414a-488d-9792-a0541c2f6776"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "gpZ6KZrOyKYS",
    "outputId": "a7836df2-414a-488d-9792-a0541c2f6776"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNo3vDyUehSR"
   },
   "outputs": [],
   "source": [
    "# If your version is 4.0.0 or later, this code will raise `AttributeError: Can't get attribute 'Vocab'...`\n",
    "# You can load an old version of Gensim or skip to FastText.\n",
    "model=gensim.models.Word2Vec.load('../data/1992embeddings_hs_new3.sg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsEgVPQ5ehSR"
   },
   "source": [
    "## analogy\n",
    "\n",
    "King+man-Queen? A few examples based on a corpus of Chinese news. \n",
    "\n",
    "First, location analogy: **province -> capital**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGV-S0jwehSR",
    "outputId": "c83a15d9-0397-4fc0-f96a-5ec2bd2484e1"
   },
   "outputs": [],
   "source": [
    "mm = model.wv.most_similar(positive=[u'',u''], negative=[u'']) # Changsha + Shaanxi - Hunan\n",
    "for m in mm:\n",
    "    print(m[0],m[1])\n",
    "    print(\"Xi'an\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y305qZtvehSR",
    "outputId": "1e554c9b-86a4-474a-dada-9deaaee758e6"
   },
   "outputs": [],
   "source": [
    "mm = model.wv.most_similar(positive=[u'',u''], negative=[u'']) # Guangzhou + Hubei - Guangdong\n",
    "for m in mm:\n",
    "    print(m[0],m[1])\n",
    "    print(\"Wuhan\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE5L8DqPehSR"
   },
   "source": [
    "Second, location analogy: **country -> capital**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVGW1hlMehSR",
    "outputId": "dbe2c395-f7ee-4225-ca80-ff49200c72c3"
   },
   "outputs": [],
   "source": [
    "mm = model.wv.most_similar(positive=[u'',u''], negative=[u'']) # Tokyo + US - Japan\n",
    "for m in mm:\n",
    "    print(m[0],m[1])\n",
    "    print(\"(Washington DC)\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AYFM1ukehSS"
   },
   "source": [
    "## similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pTexZipehSS",
    "outputId": "645e5337-7b3a-4dfb-c679-7f7239d285cb"
   },
   "outputs": [],
   "source": [
    "word = u''  #socialism\n",
    "ss = model.wv.similar_by_word(word,topn=10)\n",
    "print(\"the most similar words to \" + word + \" is: \")\n",
    "for s in ss:\n",
    "    print(s[0])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Edcvj0knehSS",
    "outputId": "cfeab2f2-1f68-443d-b514-4a5f86929468"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "word = u''  # corn\n",
    "ss = model.wv.similar_by_word(word,topn=10)\n",
    "print(\"the most similar words to \" + word + \" is: \")\n",
    "for s in ss:\n",
    "    print(s[0])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e_V1tGDpJNX"
   },
   "source": [
    "## Adding more context - FastText\n",
    "\n",
    "Since the original word2vec paper, there have followed a slew of word embedding related methods which innovate and build on them in many ways. One popular extension is FastText ([Bojanowski et al. 2017](https://arxiv.org/abs/1607.04606)), which uses sub-words to generate its vectors. Using subwords means that it is powerful in dealing with unknown words and sparse languages that otherwise have a rich morphological structure. These sub-words are incorporated into the previously skipgram and CBOW methods. For example, if the word is which, it is represented as the word itself along with a bag of constituent n-grams. If n=3, the representation looks like <wh, whi, hic, ich, ch>, and we learn a representation for each of these constituents, with the word which taking on the average value of these constituents. \n",
    "\n",
    "FastText can be used either via Gensim or the official package, and primarily has two functions - word representations, and text classification (Joulin et al. 2017). See below for code using the FastText package for playing with word representations, taken from their word representations tutorial (https://fasttext.cc/docs/en/unsupervised-tutorial.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9hXjY0FPpNVq",
    "outputId": "e8a3a0c5-c360-416e-82ac-29f519bbba8c"
   },
   "outputs": [],
   "source": [
    "! pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqNJ5jWgpSCB"
   },
   "source": [
    "### Representations with FastText\n",
    "\n",
    "We will follow the instructions in the FastText tutorial to prepare our data, (Wikipedia). You can either follow the instructions on the page for setting the data, or download/copy it to drive it from this [Google Drive link](https://drive.google.com/file/d/12T3nNzf0a7tdhm1lVyfz9Ix9XVITFMCP/view?usp=sharing). \n",
    "\n",
    "In this example, we will be training a model - it is also possible to download and use the many pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4iWKbb8s8Na"
   },
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_1B3ckRpUP1"
   },
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t37MtAB4JYUb"
   },
   "outputs": [],
   "source": [
    "# Replace with the path for where you have placed the 'fil9' file.\n",
    "file_address = \"/content/drive/MyDrive/fil9\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7YqmoQEI360"
   },
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised(file_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVdsL1O-t171"
   },
   "source": [
    "While FastText is running, the progress and estimated time to completion is shown on your screen. Once the training finishes, the model variable contains information on the trained model, which you can use for querying:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2NARfwxI6FL"
   },
   "outputs": [],
   "source": [
    "model.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7y1Qr95t7if"
   },
   "source": [
    "It returns all words in the vocabulary, sorted by decreasing frequency. We can get the word vector by:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lxZJCAst4n1"
   },
   "outputs": [],
   "source": [
    "model.get_word_vector(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXjgMWFCt4lE"
   },
   "outputs": [],
   "source": [
    "model.save_model(\"result/fil9.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CznMfOVbt4iG"
   },
   "outputs": [],
   "source": [
    "# when we want to use the model again\n",
    "# model = fasttext.load_model(\"result/fil9.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3H_c9lVq0WQ"
   },
   "source": [
    "Let's now print some vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsAsfHyNt4e4"
   },
   "outputs": [],
   "source": [
    "[model.get_word_vector(x) for x in [\"asparagus\", \"pidgey\", \"yellow\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAXTmBV1q_Xv"
   },
   "source": [
    "A nice feature is that you can also query for words that did not appear in your data! Indeed words are represented by the sum of their substrings. As long as the unknown word is made of known substrings, there is a representation of it!\n",
    "\n",
    "As an example let's try with a misspelled word:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmWu9IZsrCl-"
   },
   "outputs": [],
   "source": [
    "model.get_word_vector(\"enviroment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FastText here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 1*</font>\n",
    "\n",
    "<font color=\"red\">Construct cells immediately below this that embed documents related to your final project using at least two different specification of `word2vec` and/or `fasttext`, and visualize them each with two separate visualization layout specifications (e.g., TSNE, PCA). Then interrogate critical word vectors within your corpus in terms of the most similar words, analogies, and other additions and subtractions that reveal the structure of similarity and difference within your semantic space. What does this pattern reveal about the semantic organization of words in your corpora? Which estimation and visualization specification generate the most insight and appear the most robustly supported and why? \n",
    "\n",
    "<font color=\"red\">***Stretch***: Explore different vector calculations beyond addition and subtraction, such as multiplication, division or some other function. What does this exploration reveal about the semantic structure of your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Instead of just looking at just how words embed within in the space, we can look at how the different documents relate to each other within the space. First lets load our data--abstracts of most U.S. physics papers from the 1950s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsDF = pd.read_csv('../data/APSabstracts1950s.csv', index_col = 0)\n",
    "apsDF[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load these as documents into Word2Vec, but first we need to normalize and pick some tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['photomagnetoelectric', 'quantum', 'boltzmann', 'proton', 'positron', 'feynman', 'classical', 'relativity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsDF['tokenized_words'] = apsDF['abstract'].apply(lambda x: lucem_illud.word_tokenize(x))\n",
    "apsDF['normalized_words'] = apsDF['tokenized_words'].apply(lambda x: lucem_illud.normalizeTokens(x, lemma=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedDocs = []\n",
    "for index, row in apsDF.iterrows():\n",
    "    #Just doing a simple keyword assignment\n",
    "    docKeywords = [s for s in keywords if s in row['normalized_words']]\n",
    "    docKeywords.append(row['copyrightYear'])\n",
    "    docKeywords.append(row['doi']) #This lets us extract individual documnets since doi's are unique\n",
    "    taggedDocs.append(gensim.models.doc2vec.LabeledSentence(words = row['normalized_words'], tags = docKeywords))\n",
    "apsDF['TaggedAbstracts'] = taggedDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a Doc2Vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V = gensim.models.doc2vec.Doc2Vec(apsDF['TaggedAbstracts'], size = 100) #Limiting to 100 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get vectors for the tags/documents, just as we did with words. Documents are actually the centroids (high dimensional average points) of their words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apsD2V.docvecs[1952]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words can still be accessed in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V['atom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still use the ``most_similar`` command to perform simple semantic equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.most_similar(positive = ['atom','electrons'], negative = ['electron'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. **Electron** is to **electrons** as **atom** is to **atoms**. Another way to understand this, developed below is: **electrons - electron** induces a singular to plural dimension, so when we subtract **electron** from **atom** and add **electrons**, we get **atoms**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.most_similar(positive = ['einstein','law'], negative = ['equation'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words **Einstein** minus **equation** plus **law** equals **Meissner**--Walthur Meissner studied mechanical engineering and physics ... and was more likely to produce a \"law\" than a \"equation\", like the Meissner effect, the damping of the magnetic field in superconductors. If we built our word-embedding with a bigger corpus like the entire arXiv, a massive repository of physics preprints, we would see many more such relationships like **gravity - Newton + Einstein = relativity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute all of these *by hand*--explicitly wth vector algebra: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.pairwise.cosine_similarity(apsD2V['electron'].reshape(1,-1), apsD2V['positron'].reshape(1,-1))\n",
    "#We reorient the vectors with .reshape(1, -1) so that they can be computed without a warning in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the doc2vec model, the documents have vectors just as the words do, so that we can compare documents with each other and also with words (similar to how a search engine locates a webpage with a query). First, we will calculate the distance between a word and documents in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.docvecs.most_similar([ apsD2V['electron'] ], topn=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we search for the first of these on the web (these are doi codes), we find the following...a pretty good match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"../data/PhysRev.98.875.jpg\", width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go the other way around and find words most similar to this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.most_similar( [ apsD2V.docvecs['10.1103/PhysRev.98.875'] ], topn=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even look for documents most like a query composed of multiple words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.docvecs.most_similar([ apsD2V['electron']+apsD2V['positron']+apsD2V['neutron']], topn=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot some words and documents against one another with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmapMatrix = []\n",
    "for tagOuter in keywords:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in keywords:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrix.append(column)\n",
    "heatmapMatrix = np.array(heatmapMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrix, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrix.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrix.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(keywords, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(keywords, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a heatmap of similarities between the first ten documents in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetDocs = apsDF['doi'][:10]\n",
    "\n",
    "heatmapMatrixD = []\n",
    "\n",
    "for tagOuter in targetDocs:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in targetDocs:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrixD.append(column)\n",
    "heatmapMatrixD = np.array(heatmapMatrixD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrixD, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrixD.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrixD.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(targetDocs, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(targetDocs, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a heatmap of similarities between the first ten documents and our keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmapMatrixC = []\n",
    "\n",
    "for tagOuter in targetDocs:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in keywords:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrixC.append(column)\n",
    "heatmapMatrixC = np.array(heatmapMatrixC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrixC, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrixC.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrixC.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(keywords, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(targetDocs, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the model in case we would like to use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apsD2V.save('apsW2V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can later load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apsD2V = gensim.models.word2vec.Word2Vec.load('data/apsW2V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 2*</font>\n",
    "\n",
    "<font color=\"red\">Construct cells immediately below this that embed documents related to your final project using `doc2vec`, and explore the relationship between different documents and the word vectors you analyzed in the last exercise. Consider the most similar words to critical documents, analogies (doc _x_ + word _y_), and other additions and subtractions that reveal the structure of similarity and difference within your semantic space. What does this pattern reveal about the documentary organization of your semantic space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also project word vectors to an arbitray semantic dimension. To demonstrate this possibility, let's first load a model trained with New York Times news articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nytimes_model = gensim.models.KeyedVectors.load_word2vec_format('../data/nytimes_cbow.reduced.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can visualize with dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words to create dimensions\n",
    "tnytTargetWords = ['man','him','he', 'woman', 'her', 'she', 'black','blacks','African', 'white', 'whites', 'Caucasian', 'rich', 'richer', 'richest', 'expensive', 'wealthy', 'poor', 'poorer', 'poorest', 'cheap', 'inexpensive']\n",
    "#words we will be mapping\n",
    "tnytTargetWords += [\"doctor\",\"lawyer\",\"plumber\",\"scientist\",\"hairdresser\", \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \"banker\",\"poet\",\"nurse\", \"steak\", \"bacon\", \"croissant\", \"cheesecake\", \"salad\", \"cheeseburger\", \"vegetables\", \"beer\", \"wine\", \"pastry\", \"basketball\", \"baseball\", \"boxing\", \"softball\", \"volleyball\", \"tennis\", \"golf\", \"hockey\", \"soccer\"]\n",
    "\n",
    "\n",
    "wordsSubMatrix = []\n",
    "for word in tnytTargetWords:\n",
    "    wordsSubMatrix.append(nytimes_model[word])\n",
    "wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "#wordsSubMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaWordsNYT = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_dataNYT = pcaWordsNYT.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWordsNYT = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_dataNYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWordsNYT[:, 0], tsneWordsNYT[:, 1], alpha = 0) #Making the points invisible\n",
    "for i, word in enumerate(tnytTargetWords):\n",
    "    ax.annotate(word, (tsneWordsNYT[:, 0][i],tsneWordsNYT[:, 1][i]), size =  50 * (len(tnytTargetWords) - i) / len(tnytTargetWords))\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some convenient functions for getting dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vector):\n",
    "    normalized_vector = vector / np.linalg.norm(vector)\n",
    "    return normalized_vector\n",
    "\n",
    "def dimension(model, positives, negatives):\n",
    "    diff = sum([normalize(model[x]) for x in positives]) - sum([normalize(model[y]) for y in negatives])\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate three dimensions: gender, race, and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender = dimension(nytimes_model, ['man','him','he'], ['woman', 'her', 'she'])\n",
    "Race = dimension(nytimes_model, ['black','blacks','African'], ['white', 'whites', 'Caucasian'])\n",
    "Class = dimension(nytimes_model, ['rich', 'richer', 'richest', 'expensive', 'wealthy'], ['poor', 'poorer', 'poorest', 'cheap', 'inexpensive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Occupations = [\"doctor\",\"lawyer\",\"plumber\",\"scientist\",\"hairdresser\", \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \"banker\",\"poet\",\"nurse\"]\n",
    "\n",
    "Foods = [\"steak\", \"bacon\", \"croissant\", \"cheesecake\", \"salad\", \"cheeseburger\", \"vegetables\", \"beer\", \"wine\", \"pastry\"]\n",
    "\n",
    "Sports  = [\"basketball\", \"baseball\", \"boxing\", \"softball\", \"volleyball\", \"tennis\", \"golf\", \"hockey\", \"soccer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to project words in a word list to each of the three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDF(model, word_list):\n",
    "    g = []\n",
    "    r = []\n",
    "    c = []\n",
    "    for word in word_list:\n",
    "        g.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Gender.reshape(1,-1))[0][0])\n",
    "        r.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Race.reshape(1,-1))[0][0])\n",
    "        c.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Class.reshape(1,-1))[0][0])\n",
    "    df = pd.DataFrame({'gender': g, 'race': r, 'class': c}, index = word_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCCdf = makeDF(nytimes_model, Occupations) \n",
    "Fooddf = makeDF(nytimes_model, Foods)\n",
    "Sportsdf = makeDF(nytimes_model, Sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some useful functions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Coloring(Series):\n",
    "    x = Series.values\n",
    "    y = x-x.min()\n",
    "    z = y/y.max()\n",
    "    c = list(plt.cm.rainbow(z))\n",
    "    return c\n",
    "\n",
    "def PlotDimension(ax,df, dim):\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_title(dim, fontsize = 20)\n",
    "    colors = Coloring(df[dim])\n",
    "    for i, word in enumerate(df.index):\n",
    "        ax.annotate(word, (0, df[dim][i]), color = colors[i], alpha = 0.6, fontsize = 12)\n",
    "    MaxY = df[dim].max()\n",
    "    MinY = df[dim].min()\n",
    "    plt.ylim(MinY,MaxY)\n",
    "    plt.yticks(())\n",
    "    plt.xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the occupational words in each of the three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, OCCdf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, OCCdf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, OCCdf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, Fooddf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, Fooddf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, Fooddf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, Sportsdf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, Sportsdf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, Sportsdf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 3*</font>\n",
    "\n",
    "<font color=\"red\">Construct cells immediately below this that embed documents related to your final project, then generate meaningful semantic dimensions based on your theoretical understanding of the semantic space (i.e., by subtracting semantically opposite word vectors) and project another set of word vectors onto those dimensions. Interpret the meaning of these projections for your analysis. Which of the dimensions you analyze explain the most variation in the projection of your words and why? \n",
    "\n",
    "<font color=\"red\">***Stretch***: Average together multiple antonym pairs to create robust semantic dimensions. How do word projections on these robust dimensions differ from single-pair dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Score Function\n",
    "\n",
    "The score function is a simple calculation developed by [Matt Taddy](https://arxiv.org/pdf/1504.07295.pdf) to calculate the likelihood that a given text would have been generated by a word-embedding model by summing the inner product between each pair of the text's word vectors. This relies on a few data files that are not in the git repo due to their size please download and unzip [this](https://github.com/Computational-Content-Analysis-2018/Upcoming/raw/master/data/supplement.zip) (472MB) file in the data directory.\n",
    "\n",
    "Here, we explore this using a model trained with millions of resumes from the CareerBuilder website (we can't share the private resumes...but we can share a model built with them :-):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_model  = gensim.models.word2vec.Word2Vec.load('../data/resumeAll.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the vacabularies of this model by building a word-index map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = resume_model.wv.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just load the sample and take a look at it. The sentences in each job description are already tokenized and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF = pd.read_csv('../data/SampleJobAds.csv', index_col = False)\n",
    "#We need to convert the last couple columns from strings to lists\n",
    "sampleDF['tokenized_sents'] = sampleDF['tokenized_sents'].apply(lambda x: eval(x))\n",
    "sampleDF['normalized_sents'] = sampleDF['normalized_sents'].apply(lambda x: eval(x))\n",
    "sampleDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to calculate the likelihood of each job description. The idea is borrowed from [Matt Taddy](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/deepir.ipynb), who shows how a document can be characterized as the inner product of the distance between its words. In other words, this analysis will show which job ads are most likely to find an appropriate pool of workers in the resume bank that generated our word embedding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adprob(ad, model):\n",
    "    sen_scores = model.score(ad, len(ad))\n",
    "    ad_score = sen_scores.mean()\n",
    "    return ad_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this function to every job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF['likelihood'] = sampleDF['normalized_sents'].apply(lambda x: adprob(x, resume_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the top 5 job descriptions that have the highest likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ad in sampleDF.sort_values(by = 'likelihood', ascending = False)['jobDescription'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the bottom 5 job descriptions that have the lowest likelihood to be matched by the resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ad in sampleDF.sort_values(by = 'likelihood')['jobDescription'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for phrases corresponding to job skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adprob([[\"python\", \"programming\"]], resume_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adprob([[\"julia\", \"programming\"]], resume_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic programming appears to be more likely in this pool of resumes than python programming. \n",
    "\n",
    "We can also do some simple statistics. Unfortunately, we don't have a large sample here. Nevertheless, let's first look at the mean likelihood score of each hiring organization. Some organizations will do well to hire on CareerBuilder...while others will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF.groupby(\"hiringOrganization_organizationName\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the mean likelihood of each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF.groupby(\"jobLocation_address_region\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would increase the sample size if you want to do a more serious study.\n",
    "\n",
    "## <font color=\"red\">*Exercise 4a*</font>\n",
    "\n",
    "<font color=\"red\">**Do only 4a or 4b.** Construct cells immediately below this that calculate the scores for a small sample of documents from outside your corpus to identify which are *closest* to your corpus. Then calculate the scores for a few phrases or sentences to identify the ones most likely to have appeared in your corpus. Interrogate patterns associated with these document/phrase scores (e.g., which companies produced job ads most or least likely to find jobseekers in the resume corpus?) What do these patterns suggest about the boundaries of your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Change or Difference\n",
    "\n",
    "Below is code that aligns the dimensions of multiple embeddings arrayed over time or some other dimension and allow identification of semantic chanage as the word vectors change their loadings for focal words. This code comes from the approach piloted at Stanford by William Hamilton, Daniel Jurafsky and Jure Lescovec [here](https://arxiv.org/pdf/1605.09096.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_syn0norm(model):\n",
    "    \"\"\"since syn0norm is now depricated\"\"\"\n",
    "    return (model.wv.syn0 / np.sqrt((model.wv.syn0 ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)\n",
    "\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "    (With help from William. Thank you!)\n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "    base_embed = copy.copy(base_embed)\n",
    "    other_embed = copy.copy(other_embed)\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the embedding matrices\n",
    "    base_vecs = calc_syn0norm(in_base_embed)\n",
    "    other_vecs = calc_syn0norm(in_other_embed)\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one\n",
    "    # i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
    "    other_embed.wv.syn0norm = other_embed.wv.syn0 = (calc_syn0norm(other_embed)).dot(ortho)\n",
    "    return other_embed\n",
    "    \n",
    "def intersection_align_gensim(m1,m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.vocab.keys())\n",
    "    vocab_m2 = set(m2.wv.vocab.keys())\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1&vocab_m2\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.vocab[w].count + m2.wv.vocab[w].count,reverse=True)\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1,m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.vocab[w].index for w in common_vocab]\n",
    "        old_arr = calc_syn0norm(m)\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.syn0norm = m.wv.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        old_vocab = m.wv.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.wv.vocab = new_vocab\n",
    "\n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explore this, let's get some data that follows a time trend. We'll look at conference proceedings from the American Society for Clinical Oncologists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascoDF = pd.read_csv(\"../data/ASCO_abstracts.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for wor2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascoDF['tokenized_sents'] = ascoDF['Body'].apply(lambda x: [lucem_illud.word_tokenize(s) for s in lucem_illud.sent_tokenize(x)])\n",
    "ascoDF['normalized_sents'] = ascoDF['tokenized_sents'].apply(lambda x: [lucem_illud.normalizeTokens(s) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascoDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be creating many embeddings so we have created this function to do most of the work. It creates two collections of embeddings, one the original and one the aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareModels(df, category, text_column_name='normalized_sents', sort = True, embeddings_raw={}):\n",
    "    \"\"\"If you are using time as your category sorting is important\"\"\"\n",
    "    if len(embeddings_raw) == 0:\n",
    "        embeddings_raw = rawModels(df, category, text_column_name, sort)\n",
    "    cats = sorted(set(df[category]))\n",
    "    #These are much quicker\n",
    "    embeddings_aligned = {}\n",
    "    for catOuter in cats:\n",
    "        embeddings_aligned[catOuter] = [embeddings_raw[catOuter]]\n",
    "        for catInner in cats:\n",
    "            embeddings_aligned[catOuter].append(smart_procrustes_align_gensim(embeddings_aligned[catOuter][-1], embeddings_raw[catInner]))\n",
    "    return embeddings_raw, embeddings_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawModels(df, category, text_column_name='normalized_sents', sort = True):\n",
    "    embeddings_raw = {}\n",
    "    cats = sorted(set(df[category]))\n",
    "    for cat in cats:\n",
    "        #This can take a while\n",
    "        print(\"Embedding {}\".format(cat), end = '\\r')\n",
    "        subsetDF = df[df[category] == cat]\n",
    "        #You might want to change the W2V parameters\n",
    "        embeddings_raw[cat] = gensim.models.word2vec.Word2Vec(subsetDF[text_column_name].sum())\n",
    "    return embeddings_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawEmbeddings, comparedEmbeddings = compareModels(ascoDF, 'Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawEmbeddings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compare them across all permutions so we will define another function to help, we will be using 1 - cosine similarity as that gives a more intitive range of 0-2 with low values meaning little change and high meaning lots of change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDivergenceDF(word, embeddingsDict):\n",
    "    dists = []\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    dists = {}\n",
    "    print(word)\n",
    "    for cat in cats:\n",
    "        dists[cat] = []\n",
    "        for embed in embeddingsDict[cat][1:]:\n",
    "            dists[cat].append(np.abs(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cat][0][word], axis = 0),\n",
    "                                                                             np.expand_dims(embed[word], axis = 0))[0,0]))\n",
    "    return pd.DataFrame(dists, index = cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a couple words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = 'breast'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = 'combination'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDiverence(word, embeddingsDict):\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    \n",
    "    dists = []\n",
    "    for embed in embeddingsDict[cats[0]][1:]:\n",
    "        dists.append(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cats[0]][0][word], axis = 0), np.expand_dims(embed[word], axis = 0))[0,0])\n",
    "    return sum(dists)\n",
    "\n",
    "def findMostDivergent(embeddingsDict):\n",
    "    words = []\n",
    "    for embeds in embeddingsDict.values():\n",
    "        for embed in embeds:\n",
    "            words += list(embed.wv.vocab.keys())\n",
    "    words = set(words)\n",
    "    print(\"Found {} words to compare\".format(len(words)))\n",
    "    return sorted([(w, findDiverence(w, embeddingsDict)) for w in words], key = lambda x: x[1], reverse=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDivergences = findMostDivergent(comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most divergent words are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDivergences[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the least:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDivergences[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = wordDivergences[0][0]\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = wordDivergences[-1][0]\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COHA\n",
    "\n",
    "COHA is a historial dataset so it ends up being a good choice to try the same analysis we just did across three different time periods.\n",
    "\n",
    "We will be using the same data loading procedure as the last notebook, so go through the process to make sure you know what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_address = \"/Users/bhargavvader/Downloads/Academics_Tech/corpora/COHA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coha_texts = lucem_illud.loadDavies(corpora_address, return_raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool - let us now create our different epochs. This is an important step: I will be using the same 5 epochs I did in the DTM example, but you are recommended to play around with this. I will create a dataframe which logs the year and the genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coha_df = pd.DataFrame(columns=[\"Year\", \"Genre\", \"Epoch\", \"normalized sents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in coha_texts:\n",
    "    genre, year, id_ = article.split(\"_\")\n",
    "\n",
    "    year = int(year)\n",
    "    \n",
    "    if year > 1810 and year < 1880:\n",
    "        epoch = 0\n",
    "    if year >= 1880 and year < 1913:\n",
    "        epoch = 1\n",
    "    if year >= 1913 and year < 1950:\n",
    "        epoch = 2\n",
    "    if year >= 1950 and year < 1990:\n",
    "        epoch = 3\n",
    "    if year >= 1990:\n",
    "        epoch = 4\n",
    "    \n",
    "    try:\n",
    "        if len(coha_texts[article][2]) < 1500000:\n",
    "            coha_df.loc[id_] = [year, genre, epoch, lucem_illud.normalizeTokens(coha_texts[article][2].decode(\"utf-8\"), lemma=False)]\n",
    "    except TypeError:\n",
    "        continue\n",
    "    except IndexError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now arrange our word embeddings by either year, genre, or epoch, and see how the words in each of those contexts change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawEmbeddings_epoch = rawModels(coha_df, 'Epoch', text_column_name='normalized sents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawEmbeddings_genre = rawModels(coha_df, 'Genre', text_column_name='normalized sents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the raw embeddings for epoch and genre. You can test out the previous analysis on words of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch, embedding in enumerate(rawEmbeddings_epoch):\n",
    "    model = rawEmbeddings_epoch[embedding]\n",
    "    name = \"embedding_epoch_\" + str(epoch)\n",
    "    model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for embedding in rawEmbeddings_genre:\n",
    "#     model = rawEmbeddings_genre[embedding]\n",
    "#     name = \"embedding_genre_\" + embedding\n",
    "#     model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_embeddings(address, kind):\n",
    "    rawEmbeddings = {}\n",
    "    for file in os.listdir(address):\n",
    "        if \"embedding_\"+kind in file:\n",
    "            e, kind_, kind_type = file.split(\"_\")\n",
    "            kind_type = eval(kind_type)\n",
    "            rawEmbeddings[kind_type] = Word2Vec.load(file)\n",
    "    return rawEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawEmbeddings_genre_load = file_to_embeddings(\".\", \"genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawEmbeddings_epoch_load = file_to_embeddings(\".\", \"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawEmbeddings_epoch_load.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawEmbeddings_epoch, compared_epoch = compareModels(coha_df, 'Epoch', text_column_name='normalized sents', embeddings_raw=rawEmbeddings_epoch_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawEmbeddings_genre, compared_genre = compareModels(coha_df, 'Genre', text_column_name='normalized sents', embeddings_raw=rawEmbeddings_genre_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have access to the epoch wise embeddings, and the code to train models genre wise (commented out). You can use the original embeddings, the compared embeddings and such to perform the analysis we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 4b*</font>\n",
    "\n",
    "<font color=\"red\">**Do only 4a or 4b.** Construct cells immediately below this that align word embeddings over time or across domains/corpora. Interrogate the spaces that result and ask which words changed most and least over the entire period or between contexts/corpora. What does this reveal about the social game underlying your space? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is much more we do not have time to cover on word embeddings. If you are interested in other ways to align word embeddings, take a look at the [Dynamic Word Embeddings (DTM) section from the Thinking with Deep Learning course](https://colab.research.google.com/drive/1RAiI3BIL1X9D4gzZ0rZdIJjkNNicIuKE?usp=sharing#scrollTo=COS_n2RFCJNk) or using the more recent [Temporal Word Embeddings with a Compass (TWEC) package](https://github.com/valedica/twec). There is also a useful section on [debiasing word embeddings](https://colab.research.google.com/drive/1RAiI3BIL1X9D4gzZ0rZdIJjkNNicIuKE?usp=sharing#scrollTo=JHQ--EsWoxGM), such as the famous, [\"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\"](https://arxiv.org/abs/1607.06520) paper. Below, we include an optional section on topic modeling with word embeddings, which could be useful for final projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeCZlXXfvUPs"
   },
   "source": [
    "## Optional: Topic modeling with word embeddings\n",
    "\n",
    "Recently computer scientists have developed methods to cluster word embeddings, which can be viewed as a topic model, an embedding-based version of conventional topic models that use the document-term matrix (e.g., LDA). One method is Discourse Atoms, first described by Princeton NLP researchers ([Arora et al. 2018](https://arxiv.org/abs/1601.03764)). This uses k-SVD, a generalization of the k-means clustering algorithm to identify topic-like vectors in the n-dimensional word embedding space. Below is code adapted from the first social science paper using Discourse Atoms, [Arseniev-Koehler et al. 2021](https://osf.io/preprints/socarxiv/nkyaq/). It takes as input _gensim_ word vectors.\n",
    "\n",
    "You are not required to implement this, but for class projects or your own research, this can be more useful than conventional topic models. It runs faster, produces more detailed topics, and in general makes use of more information (i.e., word order within a document) than do conventional topic models. Note there are at least 4 other papers with methods for word embedding clusters:\n",
    "\n",
    "- Xun, Li, Zhao, Gao, and Zhang 2017: [multivariate Gaussian distributions](https://www.ijcai.org/proceedings/2017/588)\n",
    "- Dieng, Ruiz, and Blei 2019: [\"Embedding Topic Model (ETM)\"](https://arxiv.org/abs/1907.04907)\n",
    "- Angelov 2020: [\"Top2Vec\"](https://arxiv.org/abs/2008.09470)\n",
    "- Sia, Dalmia, and Mielke 2020: [(spherical) k-means, k-medoids, von Mises-Fisher Models, Gaussian Mixture Models](https://arxiv.org/abs/2004.14914)\n",
    "\n",
    "Let's implement the Discourse Atoms method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBHMqKnk_JkC"
   },
   "outputs": [],
   "source": [
    "# from gensim.test.utils import datapath \n",
    "# import re\n",
    "# import string, re\n",
    "# import cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rhM3yfuALJQ",
    "outputId": "a25f6c22-0225-420b-9899-6bb4574c585d"
   },
   "outputs": [],
   "source": [
    "# The ksvd package has a convenient Approximate k-SVD function.\n",
    "!pip install ksvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5y1spof_MCs"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities #calc all similarities at once, from http://radimrehurek.com/gensim/tut3.html\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from random import seed, sample\n",
    "from ksvd import ApproximateKSVD #pip or conda install ksvd #this is key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zietHqDPF5uT"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFYA73V4GH2Y"
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgH-NiNgvZMz",
    "outputId": "f0b7bab2-d83d-4379-fd82-8f8a97e21168"
   },
   "outputs": [],
   "source": [
    "# Load a gensim word2vec model\n",
    "w2v = senReleasesW2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjAyx16xvccJ"
   },
   "source": [
    "#### k-SVD\n",
    "\n",
    "We then perform a K-SVD on the word embedding matrix to learn topics in such a way where each word-vector is represented as a spare linear combination of topics. To generate a good representation of the original word vector space, we want to minimize the difference between our word vectors and the vectors generated as a linear combination of topics. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11xyYrXOvgJw"
   },
   "outputs": [],
   "source": [
    "#### TRAIN MODEL:\n",
    "\n",
    "#n_comp: Number of topics (i.e., atoms, or dictionary elements)\n",
    "#n_nonzeros: Number of nonzero coefficients to target (how many atoms each word can load onto)\n",
    "            \n",
    "def do_aksvd(w2vmodel, n_comp, n_nonzeros, save=False, savelocation='/content/aksvd_models/'): \n",
    "    #https://github.com/nel215/ksvd #takes about 2 min on Alina's laptop for 30 atoms \n",
    "    aksvd_t = ApproximateKSVD(n_components=n_comp, transform_n_nonzero_coefs=n_nonzeros) #also may adjuste n iter which is default at 10, and tolerance for error which is default at  tol=1e-6 #n_components is number of discourse atoms, since vocab size is smallish, keep this fewer. transform_n is the number of atoms (components) that a word can be a linear combo of\n",
    "    dictionary_t = aksvd_t.fit(w2vmodel.wv.vectors).components_ # Dictionary is the matrix of discourse atoms. \n",
    "    alpha_t = aksvd_t.transform(w2vmodel.wv.vectors) #get the alphas, which are the \"weights\" of each word on a discourse atoms\n",
    "\n",
    "    if save==True:\n",
    "        outfile = open(str(savelocation) + '200d_' + str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_aksvd_nvdrsdf20','wb')\n",
    "        pickle.dump(aksvd_t,outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "        outfile = open(str(savelocation) + '200d_' +str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_dictionary_nvdrsdf20','wb')\n",
    "        pickle.dump(dictionary_t,outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "        outfile = open(str(savelocation) + '200d_' + str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_alpha_nvdrsdf20','wb')\n",
    "        pickle.dump(alpha_t,outfile)\n",
    "        outfile.close()\n",
    "    return(dictionary_t, alpha_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEX-yfOQEDKH"
   },
   "source": [
    "Two quick quality checks. These are useful to choose the number of atoms in the dictionary (i.e., number of topics): $R^2$ and Topic Diversity\n",
    "\n",
    "Useful to look at product of the two since $R^2$ tends to increase with higher # topics, as Topic Diversity decreases. Intuition: more topics can better explain the original semantic space, but also then these topics are less distinct from one another. As a result, we typically want a balance between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObzUO-JZEEJS"
   },
   "outputs": [],
   "source": [
    "def reconst_qual(w2vmodel, dictionary_mat, alpha_mat):\n",
    "    #reconstruct the word vectors\n",
    "    reconstructed = alpha_mat.dot(dictionary_mat) #reconstruct word vectors and add back in mean(?). but note that reconstructed norm is still around 0-1, not 1, is that an issue?\n",
    "    #e1 = norm(w2vmodel.wv.vectors - reconstructed) #total reconstruction error, larger means MORE error. norm as specified here takes frobenius norm of error matrix.\n",
    "\n",
    "\n",
    "    #total VARIANCE in the data: sum of squares \n",
    "    squares3= w2vmodel.wv.vectors-np.mean(w2vmodel.wv.vectors, axis=1).reshape(-1,1) #https://dziganto.github.io/data%20science/linear%20regression/machine%20learning/python/Linear-Regression-101-Metrics/\n",
    "    #sst3= np.sum([i.dot(i) for i in squares3] ) #same as below\n",
    "\n",
    "    sst3= np.sum(np.square(squares3))\n",
    "\n",
    "\n",
    "    #total sum of squared ERRORS/residuals\n",
    "    e3= [reconstructed[i]-w2vmodel.wv.vectors[i] for i in range(0,len(w2vmodel.wv.vectors))]  #https://dziganto.github.io/data%20science/linear%20regression/machine%20learning/python/Linear-Regression-101-Metrics/\n",
    "    #sse3= np.sum([i.dot(i) for i in e3] ) #same as below\n",
    "    sse3= np.sum(np.square(e3))\n",
    "\n",
    "    #R^2: 1- (SSE / SST )\n",
    "    r2= 1- (sse3 /  sst3) #https://stats.stackexchange.com/questions/184603/in-pca-what-is-the-connection-between-explained-variance-and-squared-error\n",
    "\n",
    "\n",
    "    #compute root mean square error\n",
    "    rmse=  math.sqrt(np.mean(np.square(e3)))\n",
    "\n",
    "\n",
    "\n",
    "    return(sse3, rmse, r2) #https://stats.stackexchange.com/questions/184603/in-pca-what-is-the-connection-between-explained-variance-and-squared-error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipIs45YhvhBM"
   },
   "source": [
    "#### Inferring topics from document\n",
    "\n",
    "We now use a similar approach to what we saw a little earlier, where we inverted our generative model to see which documents belong to which class - we do the same now, but with discourse atoms instead of the whole model. This process tells us the topic most likely to have generated a specific context (document). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGNQXH96vmiD"
   },
   "outputs": [],
   "source": [
    "#topic diversity (% unique words among total closest 25 words to each atom)\n",
    "def topic_diversity(w2vmodel, dictionary_mat, top_n=25):\n",
    "\n",
    "    topwords=[] #list of list, each innter list includes top N words in that topic\n",
    "\n",
    "    for i in range(0, len(dictionary_mat)): #set to number of total topics\n",
    "        topwords.extend([i[0] for i in w2vmodel.wv.similar_by_vector(dictionary_mat[i],topn=top_n)]) #set for top N words \n",
    "        #print(w2vmodel.wv.similar_by_vector(dictionary[i],topn=N))\n",
    "\n",
    "    uniquewords= set(topwords)\n",
    "    diversity = len(uniquewords)/len(topwords)\n",
    "    return(diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMuRYYmVFMCF"
   },
   "outputs": [],
   "source": [
    "dictionary, alpha = do_aksvd(w2v, 150, 5, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0njiXVfFzpH",
    "outputId": "1dd8b294-3ad9-4179-abda-60027d2ec479"
   },
   "outputs": [],
   "source": [
    "topic_diversity(w2v, dictionary, top_n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQGv2X_NFL60",
    "outputId": "fb3af92a-8484-4fdc-a820-26c728308853"
   },
   "outputs": [],
   "source": [
    "reconst_qual(w2v, dictionary, alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiCYSBTwFLvK"
   },
   "outputs": [],
   "source": [
    "#loading back in the model pieces if not already in\n",
    "\n",
    "# infile = open('../data/aksvd_models/200d_150comp5nonzeros_dictionary_nvdrsdf20','rb')\n",
    "# dictionary=pickle.load(infile)\n",
    "# infile.close()\n",
    "\n",
    "# infile = open('../data/aksvd_models/200d_150comp5nonzeros_aksvd_nvdrsdf20','rb')\n",
    "# aksvd=pickle.load(infile)\n",
    "# infile.close()\n",
    "\n",
    "# infile = open('../data/aksvd_models/200d_150comp5nonzeros_alpha_nvdrsdf20','rb')\n",
    "# alpha=pickle.load(infile)\n",
    "# infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0VgluAGRGCHi",
    "outputId": "6d47d454-e63b-4ecb-d773-fe3f39b33f3f"
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(dictionary)):\n",
    "    print(\"Discourse_Atom \" + str(i))\n",
    "    print([i[0] for i in w2v.wv.similar_by_vector(dictionary[i],topn=25)]) #what are the most similar words to the Nth\n",
    "    #print([i[0] for i in w2vmodel.wv.similar_by_vector(-dictionary[i],topn=25)]) #what are the most similar words to the Nth dicourse atom?\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ysp5BMTxGEnm",
    "outputId": "ea5510cf-f8f7-4c38-8236-fa586f3a3311"
   },
   "outputs": [],
   "source": [
    "# for a specific atom, e.g., 112th atom look at 25 most similar words:\n",
    "w2v.wv.similar_by_vector(dictionary[112],topn=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_C7I9ZFGVi9",
    "outputId": "c13ea0c7-e605-4454-fb31-3d0d0813e428"
   },
   "outputs": [],
   "source": [
    "print(w2v.wv.vocab.get('the').index, '\\n', alpha[w2vmodel.wv.vocab.get('the').index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4ZtXsVrGo-1"
   },
   "outputs": [],
   "source": [
    "#useful relevant code:\n",
    "#w2vmodel.wv.index2word[3452]\n",
    "#w2vmodel.wv.most_similar('the', topn=15)\n",
    "#np.where(alpha[w2vmodel.wv.vocab.get('the').index] != 0) #get index where the loading of a word onto discourse atoms is not 0"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
