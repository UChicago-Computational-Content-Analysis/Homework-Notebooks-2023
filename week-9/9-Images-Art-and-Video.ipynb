{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 - Images, Art & Video\n",
    "This week, we \"transcend\" text to explore analysis of sound and visual content. Trillions of digital audio, image, and video files have been generated by cell phones and distributed sensors, preserved and shared through social medial, the web, private and government administrations. In this notebook, we read in and visualize audio and image files, process them to extract relevant features and measurement, then begin to explore how to analyze and extract information from them through the same approaches to supervised and unsupervised learning we have performed thoughout the quarter with text.\n",
    "\n",
    "For this notebook we will use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install git+https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import scipy #For frequency analysis\n",
    "import scipy.fftpack\n",
    "import nltk #the Natural Language Toolkit\n",
    "import requests #For downloading our datasets\n",
    "import numpy as np #for arrays\n",
    "import pandas #gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import IPython #To show stuff\n",
    "\n",
    "#Image handling install as Pillow\n",
    "import PIL\n",
    "import PIL.ImageOps\n",
    "\n",
    "#install as scikit-image, this does the image manupulation\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import skimage.segmentation\n",
    "import skimage.filters\n",
    "import skimage.color\n",
    "import skimage.graph\n",
    "import skimage.future.graph\n",
    "\n",
    "#these three do audio handling\n",
    "import pydub #Requires ffmpeg to be installed https://www.ffmpeg.org/download.html; on a mac \"brew install ffmpeg\"\n",
    "import speech_recognition #install as speechrecognition\n",
    "import soundfile #Install as pysoundfile \n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning it may generate.\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio analysis \n",
    "\n",
    "First we will consider media that predates written language...sound and spoken language. Audio (and video) files come in two major categories, lossy or lossless. Lossless files save all information the microphone recorded. Lossy files, by contrast, drop sections humans are unlikely to notice. Recorded frequencies for both types are then typically compressed, which introduces further loss. To work with audio files, we want a format that is preferably lossless or minimally compressed. We will work with `wav` files here. Note that `mp3` is not acceptable. If you do not have `wav` files, we can use python to convert to `wav`.\n",
    "\n",
    "You might need to install ```ffmpeg``` and ```ffprobe```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplePath = '../data/audio_samples/SBC060.mp3'\n",
    "transcriptPath = '../data/audio_samples/SBC060.trn'\n",
    "\n",
    "IPython.display.Audio(samplePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using a different package to convert than the in the rest of the code\n",
    "def convertToWAV(sourceFile, outputFile, overwrite = False):\n",
    "    if os.path.isfile(outputFile) and not overwrite:\n",
    "        print(\"{} exists already\".format(outputFile))\n",
    "        return\n",
    "    #Naive format extraction\n",
    "    sourceFormat = sourceFile.split('.')[-1]\n",
    "    sound = pydub.AudioSegment.from_file(sourceFile, format=sourceFormat)\n",
    "    sound.export(outputFile, format=\"wav\")\n",
    "    print(\"{} created\".format(outputFile))\n",
    "wavPath = 'sample.wav'\n",
    "convertToWAV(samplePath, wavPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our `wav` file, notice that it is much larger than the source `mp3`. We can load it with `soundfile` and work with it as a numpy data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundArr, soundSampleRate = soundfile.read(wavPath)\n",
    "soundArr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the raw data as a column array, which contains two channels (Left and Right) of the recording device. Some files, of course, will have more columns (from more microphones). The array comprises a series of numbers that measure the location of the speaker membrane (0=resting location). By quickly and rhythmically changing the location a note can be achieved. The larger the variation from the center, the louder the sound; the faster the oscillations, the higher the pitch. (The center of the oscillations does not have to be 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundSampleRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other piece of information we get is the sample rate. This tells us how many measurements made per second, which allows us to know how long the entire recording is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numS = soundArr.shape[0] // soundSampleRate\n",
    "print(\"The sample is {} seconds long\".format(numS))\n",
    "print(\"Or {:.2f} minutes\".format(numS / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final critical parameter of sound digitization is quantisation, which consists in assigning a value to each sample according to its amplitude. These values are attributed according to a bit scale. A quantisation of 8 bit will assign amplitude values along a scale of $2^8 = 256$ states around 0. Most recording systems use a $2^{16} = 65536$ bit system. Quantisation is a rounding process, where high bit quantisation produces values close to reality with values rounded to a high number of significant digits, and low bit quantisation produces values further from reality with values rounded a low number of significants digits. Low quantisation can lead to impaired quality signal. <img src=\"../data/bitrate.png\"> This figure illustrates how digital sounds is a discrete process along the amplitude scale: a 3 bit, $2^3=8$, quantization (gray bars) gives a rough approximation of the sin wave (red line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first second of the recording:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "ax.plot(soundArr[:soundSampleRate])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 2 (Left and Right) nearly \"flat\" (or equally wavy) lines. This means that there is very little noise at this part of the recording. What variation exists is due to compression or interference and represents the slight hiss you sometimes hear in low quality recordings.\n",
    "\n",
    "Let's expand our scope and look at the first 10 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundArr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "ax.plot(soundArr[:soundSampleRate * 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see definite spikes, where each represents a word or discrete sound.\n",
    "\n",
    "To see what the different parts correspond to, we can use a transcript. Because we got this file from the [Santa Barbara Corpus of Spoken American English\n",
    "](http://www.linguistics.ucsb.edu/research/santa-barbara-corpus#Contents), we just need to load the metadata, which includes a transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTranscript(targetFile):\n",
    "    #Regex because the transcripts aren't consistent enough to use csv\n",
    "    regex = re.compile(r\"(\\d+\\.\\d+)\\s(\\d+\\.\\d+)\\s(.+:)?\\s+(.*)\")\n",
    "    dfDict = {\n",
    "        'time_start' : [],\n",
    "        'time_end' : [],\n",
    "        'speaker' : [],\n",
    "        'text' : [],\n",
    "    }\n",
    "    with open(targetFile, encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            r = re.match(regex, line)\n",
    "            dfDict['time_start'].append(float(r.group(1)))\n",
    "            dfDict['time_end'].append(float(r.group(2)))\n",
    "            if r.group(3) is None:\n",
    "                dfDict['speaker'].append(dfDict['speaker'][-1])\n",
    "            else:\n",
    "                dfDict['speaker'].append(r.group(3))\n",
    "            dfDict['text'].append(r.group(4))\n",
    "    return pandas.DataFrame(dfDict)\n",
    "\n",
    "transcriptDF = loadTranscript(transcriptPath)\n",
    "transcriptDF[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a few sub-sections. First, to make things easier, we will convert the seconds markers to sample indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to be ints for indexing, luckily being off by a couple indices doesn't matter\n",
    "transcriptDF['index_start'] = (transcriptDF['time_start'] * soundSampleRate).astype('int')\n",
    "transcriptDF['index_end'] = (transcriptDF['time_end'] * soundSampleRate).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what `'Rae and I and Sue and Buddy,'` looks like, which is the seventh row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "subSample1 = soundArr[transcriptDF['index_start'][6]: transcriptDF['index_end'][6]]\n",
    "ax.plot(subSample1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's hear what that sounds like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundfile.write('../data/audio_samples/sample1.wav', subSample1, soundSampleRate)\n",
    "IPython.display.Audio('../data/audio_samples/sample1.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see sounds in the frequency space, we can take the Fourier transform. This is a reversible mathematical transform named after the French mathematician Joseph Fourier (1768-1830) <img src=\"data/Fourier.jpg\">. The transform decomposes a time series into a sum of finite series of sine or cosine functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1FFT = scipy.fftpack.ifft(subSample1)\n",
    "N = len(sample1FFT)\n",
    "freq = scipy.fftpack.fftfreq(N, d = 1 / soundSampleRate)\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "ax.set_xlabel('Frequency ($Hz$)')\n",
    "ax.set_ylabel('Intensity')\n",
    "ax.plot(freq[:N//2], abs(sample1FFT)[:N//2]) #Only want positive frequencies\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that there are two frequencies to the 'Rae and I and Sue and Buddy' snippet: a higher pitched 'Rae and I...Sue...Buddy' (~14000 *Hz*) and the final two 'and's (one at ~6000 *Hz* and the second at ~8000 *Hz*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a sniff look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "subSample2 = soundArr[transcriptDF['index_start'][9]: transcriptDF['index_end'][9]]\n",
    "ax.plot(subSample2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very different from speech. And now let's see what that sounds like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundfile.write('../data/audio_samples/sample2.wav', subSample2, soundSampleRate)\n",
    "IPython.display.Audio('../data/audio_samples/sample2.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and in frequency space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2FFT = scipy.fftpack.ifft(subSample2)\n",
    "N = len(sample2FFT)\n",
    "freq = scipy.fftpack.fftfreq(N, d = 1 / soundSampleRate)\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "ax.plot(freq[:N//2], abs(sample2FFT)[:N//2]) #Only want positive frequencies\n",
    "ax.set_xlabel('Frequency ($Hz$)')\n",
    "ax.set_ylabel('Intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how there is not a dominant frequency for the sniff as there was for the noun phrase earlier. This means that the sniff activated noise all across the frequency spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also investigate dominant frequencies for the entire record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This can take a minute\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "fullFFT = scipy.fftpack.ifft(soundArr)\n",
    "N = len(fullFFT)\n",
    "freq = scipy.fftpack.fftfreq(N, d = 1 / soundSampleRate)\n",
    "ax.plot(freq[:N//2], abs(fullFFT)[:N//2]) #Only want positive frequencies\n",
    "ax.set_xlabel('Frequency ($Hz$)')\n",
    "ax.set_ylabel('Intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq[len(freq) // 2 -10: len(freq) // 2 + 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we capture each person's frequencies across their entire collection of statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxfreq(sample, topN = 10):\n",
    "    sampleFFT = scipy.fftpack.ifft(sample)\n",
    "    N = len(sample)\n",
    "    freqs = scipy.fftpack.fftfreq(N, d = 1 / soundSampleRate)\n",
    "    tops =  np.argpartition(abs(sampleFFT[:, 0]), -topN)[-topN:]\n",
    "\n",
    "    return np.mean(tops) \n",
    "\n",
    "freqs = []\n",
    "for i, row in transcriptDF.iterrows():\n",
    "    freqs.append(maxfreq(soundArr[row['index_start']: row['index_end']]))\n",
    "\n",
    "transcriptDF['frequency FFT'] = freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alan's speech exhibits the following frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "transcriptDF[transcriptDF['speaker'] == 'ALAN:'].plot( 'time_start', 'frequency FFT', ax = ax)\n",
    "ax.set_ylabel(\"Frequency FFT space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...while Jon's voice is **much** lower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "transcriptDF[transcriptDF['speaker'] == 'JON:'].plot( 'time_start', 'frequency FFT', ax = ax)\n",
    "ax.set_ylabel(\"Frequency FFT space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can look at them togther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = seaborn.FacetGrid(data=transcriptDF, hue='speaker', aspect = 3)\n",
    "fg.map(plt.scatter, 'time_start', 'frequency FFT').add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech-to-Text\n",
    "\n",
    "We can also do speech recognition on audio, but this requires a complex machine learning system. Luckily there are many online services to do this. We have a function that uses Google's API. There are two API's: one is free but limited; the other is commercial and you can provide the function `speechRec` with a file containing the API keys, using `jsonFile=` if you wish. For more about this look [here](https://stackoverflow.com/questions/38703853/how-to-use-google-speech-recognition-api-in-python) or the `speech_recognition` [docs](https://github.com/Uberi/speech_recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using another library so we need to use files again\n",
    "def speechRec(targetFile, language = \"en-US\", raw = False, jsonFile = ''):\n",
    "    r = speech_recognition.Recognizer()\n",
    "    if not os.path.isfile(jsonFile):\n",
    "        jsonString = None\n",
    "    else:\n",
    "        with open(jsonFile) as f:\n",
    "            jsonString = f.read()\n",
    "    with speech_recognition.AudioFile(targetFile) as source:\n",
    "        audio = r.record(source)\n",
    "    try:\n",
    "        if jsonString is None:\n",
    "            print(\"Sending data to Google Speech Recognition\")\n",
    "            dat =  r.recognize_google(audio)\n",
    "        else:\n",
    "            print(\"Sending data to Google Cloud Speech\")\n",
    "            dat =  r.recognize_google_cloud(audio, credentials_json=jsonString)\n",
    "    except speech_recognition.UnknownValueError:\n",
    "        print(\"Google could not understand audio\")\n",
    "    except speech_recognition.RequestError as e:\n",
    "        print(\"Could not request results from Google service; {0}\".format(e))\n",
    "    else:\n",
    "        print(\"Success\")\n",
    "        return dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above is of too low quality so we will be using another file `data/audio_samples/english.wav`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "\n",
    "# open up a wave\n",
    "wf = wave.open('../data/audio_samples/english.wav', 'rb')\n",
    "swidth = wf.getsampwidth()\n",
    "RATE = wf.getframerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('../data/audio_samples/english.wav', rate=RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speechRec('../data/audio_samples/english.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 1*</font>\n",
    "\n",
    "<font color=\"red\">Construct cells immediately below this that read in 10 audio files (e.g., produced on your smartphone recorder?) from at least two different speakers, which include sentences of different types (e.g., question, statement, exclamation). At least two of these should include recordings of the two speakers talking to each other (e.g., a simple question/answer). Contrast the frequency distributions of the words spoken within speaker. What speaker's voice has a higher and which has lower frequency? What words are spoken at the highest and lowest frequencies? What parts-of-speech tend to be high or low? How do different types of sentences vary in their frequency differently? When people are speaking to each other, how do their frequencies change? Whose changes more?\n",
    "    \n",
    "OR\n",
    "\n",
    "<font color=\"red\">Construct cells immediately below this that use the 10 audio files from at least two different speakers read in previously, attempt to automatically extract the words from Google, and calculate the word-error rate, as descibed in Chapter 9 from *Jurafsky & Martin*, page 334. How well does it do? Under what circumstances does it perform poorly? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Audio\n",
    "\n",
    "Today, most state of the art methods involving audio uses deep learning methods to embed audio in a high dimensional space - similar to some of the methods we have seen before for text. While earlier methods for speech feature extraction and classification used audio features such as some of what we saw earlier in this notebook (frequencies), as well as power (via the [Mel-frequency cepstrum](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) coefficients), the success of word2vec quickly inspired [speech2vec](https://arxiv.org/pdf/1803.08976.pdf) method to use these features to construct semantically coherent vectors. This was a jump from the acoustic and phenome based vectors previously calculated. Further work has lead to state-of-the-art wav2vec2, which we will explore in this section. We will be revisiting the Transformers package we saw last week, this time using them to load pre-trained wav2vec2 models for both creating a vector representation to making a transcription. In the example below we use a sample from librispeech, which is also what this model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Tokenizer, Wav2Vec2Model\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_array(batch):\n",
    "    speech, _ = sf.read(batch[\"file\"])\n",
    "    batch[\"speech\"] = speech\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(map_to_array)\n",
    "input_values = tokenizer(ds[\"speech\"][0], return_tensors=\"pt\").input_values  # Batch size 1\n",
    "hidden_states = model(input_values).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden state of the model serves as the embedding for the file. Below is the cosine similarity function - how would you use it to measure between audio embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(input_values).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = tokenizer.decode(predicted_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here how we can quite easily transcribe text straight off the model from Transformers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 2*</font>\n",
    "\n",
    "<font color=\"red\"> We've seen two ways in which we can use the wav2vec2 model from Transformers - to generate a high dimensional embedding, and to transcribe an audio file into text. All of this is done using a Deep Neural model!\n",
    "\n",
    "<font color=\"red\"> In the following cells, use these methods to conduct analysis of the 10 audio files which you have - maybe add 2 songs to the mix as well. How do the embeddings cluster the audio files? Which files are most similar to each other? How does the model perform with respect to a file recorded by you versus a song, versus the sample dataset? How can you incorporate the wav2vec2 model and other multi-modal data into your projects?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image analysis\n",
    "\n",
    "Now we will explore image files. First, we will read in a couple of images. Please change the working image and see how the resuts differ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_wiki = PIL.Image.open('../data/IMAGES/Wikimedia_Conference_2016_–_Group_photo.jpg')\n",
    "image_wikiGray = PIL.ImageOps.invert(image_wiki.convert('L'))\n",
    "\n",
    "image_AllSaints = PIL.Image.open('../data/IMAGES/AllSaintsMargaretStreet-DAVID_ILIFF.jpg')\n",
    "image_AllSaintsGray = PIL.ImageOps.invert(image_AllSaints.convert('L'))\n",
    "\n",
    "image_Soyuz = PIL.Image.open('../data/IMAGES/Soyuz.jpg')\n",
    "image_SoyuzGray = PIL.ImageOps.invert(image_Soyuz.convert('L'))\n",
    "\n",
    "image_Rock = PIL.Image.open('../data/IMAGES/Bi-crystal.jpg')\n",
    "image_RockGray = PIL.ImageOps.invert(image_Rock.convert('L'))\n",
    "\n",
    "image_flowers = PIL.Image.open('../data/IMAGES/flowers.jpg')\n",
    "image_flowersGray = PIL.ImageOps.invert(image_flowers.convert('L'))\n",
    "\n",
    "image = image_flowers\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageArr = np.asarray(image)\n",
    "imageArr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image we have loaded is a raster image, meaning it is a grid of pixels. Each pixel contains 1-4 numbers giving the amounts of color contained in it. In this case, we can see it has 3 values per pixel, these are RGB or Red, Green and Blue values. If we want to see just the red, we can look at just that array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgRatio = imageArr.shape[0] / imageArr.shape[1]\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "ax.imshow(imageArr[:,:,0], cmap='Reds') #The order is R G B, so 0 is the Reds\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgRatio = imageArr.shape[0] / imageArr.shape[1]\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "ax.imshow(imageArr[:,:,1], cmap='Greens') #The order is R G B, so 2 is the Green\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgRatio = imageArr.shape[0] / imageArr.shape[1]\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "ax.imshow(imageArr[:,:,2], cmap='Blues') #The order is R G B, so 2 is the Blue\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can look at all four together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (10, 10), ncols=2, nrows=2)\n",
    "axeIter = iter(axes.flatten())\n",
    "colours = [\"Reds\", \"Greens\", \"Blues\"]\n",
    "ax = next(axeIter)\n",
    "ax.imshow(imageArr)\n",
    "ax.axis('off')\n",
    "\n",
    "for i in range(3):\n",
    "    ax = next(axeIter)\n",
    "    ax.imshow(imageArr[:,:,i], cmap=colours[i]) #The order is R G B, so 2 is the Blue\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gray = PIL.ImageOps.invert(image.convert('L'))\n",
    "image_grayArr = np.asarray(image_gray)\n",
    "image_grayArr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grayscale image is defined by its pixel intensities (and a color image can be defined by its red, green, blue pixel intensities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgRatio = imageArr.shape[0] / imageArr.shape[1]\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "ax.imshow(image_grayArr) #No third dimension\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Nets and Images\n",
    "\n",
    "Modern image and video analysis is typically performed using deep learning implemented as layers of convolutional neural nets to classify scenes and to detect and label objects. We've already seen how deep learning is powerful for text (and audio, in this notebook). To learn more about deep learning and convolutional neural networks, spend some time with Andrew Ng's excellent [tutorial](http://ufldl.stanford.edu/tutorial/). \n",
    "\n",
    "### Some Deep Learning Resources for Images\n",
    "\n",
    "We saw how we can use PyTorch and Google Colab in last weeks homework. [This PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) demonstrates how to train the classifiers. Image captioning is another application of such deep neural nets - [this GitHub repository](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning) walks us through how to do image captioning with pytorch. Object Detection and Scene Classification are other ways deep neural nets are used for images - [this PyTorch tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) demonstrates Object Deteciton, and there are multiple ([1](https://github.com/buptchan/scene-classification), [2](https://github.com/ran337287/MRCNN-for-Scene-Classification), [3](https://github.com/zanilzanzan/FuseNet_PyTorch)]. Another popular open source image object detector is [UC Berkeley's caffe library](http://caffe.berkeleyvision.org) of trained and trainable neural nets written in C++. (Check out the [python api](https://github.com/BVLC/caffe/blob/master/python/caffe/pycaffe.py)). Scene classifiers can be built on top of caffe, such as MIT's [Places](http://places2.csail.mit.edu/demo.html). \n",
    "\n",
    "# Creating Image Vectors with Deep Learning and More\n",
    "\n",
    "While most Deep Learning methods for images are computationally heavy, there are still many ways to use the power of deep learning models (and other methods) to create useful representations of images for a variety of similarity tasks. \n",
    "Most of the time intensive operations which CNNs perform happen during the training process. Model inference (which is when we use a model to either make a class prediction or embed an image in a high dimensional space), however, doesn't take so long, and we can make use of powerful pre-trained models to help us in our content analysis.\n",
    "\n",
    "### Using Torch's pre-trained models\n",
    "\n",
    "PyTorch is not only an elegant and powerful deep learning framework, but comes especially handy with its wide range of pre-trained models and datasets which allow for easy use. In this section, we will use AlexNet, a deep learning model famous for its performance in the 2012 ImageNet competition, where it was the best performing model (it is now cited over 70,000 times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is code that will transform images to the appropriate format\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/imagenet_classes.txt') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "#organize class data so it can be mapped effectively\n",
    "# classes = classes[4:len(classes)]\n",
    "labels = [''.join([i for i in x if not i.isdigit()]) for x in classes]\n",
    "labels = [x.replace(', ',' ') for x in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/IMAGES/flowers.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path).convert('RGB')\n",
    "img_t = transformations(img)\n",
    "batch_t = torch.unsqueeze(img_t, 0)\n",
    "prediction = model(batch_t)\n",
    "\n",
    "_, indices = torch.sort(prediction, descending=True)\n",
    "percentage = torch.nn.functional.softmax(prediction, dim=1)[0] * 100\n",
    "percentages_raw = [percentage[x].item() for x in range(0, len(percentage))] \n",
    "\n",
    "top10 = [(labels[idx], percentage[idx].item()) for idx in indices[0][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that we can use this pre-trained model to gather some useful information - the top 10 labels predicted by the model, as well as the probability of it belonging to that class. It does a pretty good job too, with the flower. \n",
    "\n",
    "Let's now use a more recent model to create an embeddings: resnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Use the model object to select the desired layer\n",
    "layer = model._modules.get('avgpool')\n",
    "\n",
    "# Set model to evaluation mode\n",
    "d = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(image):\n",
    "    # code adapted from the package:\n",
    "    # https://github.com/christiansafka/img2vec/\n",
    "    # Create a PyTorch tensor with the transformed image\n",
    "    t_img = transformations(image)\n",
    "    # Create a vector of zeros that will hold our feature vector\n",
    "    # The 'avgpool' layer has an output size of 512\n",
    "    my_embedding = torch.zeros(512)\n",
    "\n",
    "    # Define a function that will copy the output of a layer\n",
    "    def copy_data(m, i, o):\n",
    "        my_embedding.copy_(o.flatten())                 # <-- flatten\n",
    "\n",
    "    # Attach that function to our selected layer\n",
    "    h = layer.register_forward_hook(copy_data)\n",
    "    # Run the model on our transformed image\n",
    "    with torch.no_grad():                               # <-- no_grad context\n",
    "        model(t_img.unsqueeze(0))                       # <-- unsqueeze\n",
    "    # Detach our copy function from the layer\n",
    "    h.remove()\n",
    "    # Return the feature vector\n",
    "    return my_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vector(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! So we just passed our transformed coffee image to the pre-trained resnet model and got a 512 dimensional image vector as the result. resent uses this vector to classify it. We can expect this image embedding to contain information of what are the coffee like characteristics of an image. \n",
    "\n",
    "The same code is wrapped up in the package img2vec_pytorch, and so far features a quick vector creation pipeline for resnet and AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from img2vec_pytorch import Img2Vec # pip install img2vec_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2vec = Img2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_vec = img2vec.get_vec(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_image = Image.open(\"../data/IMAGES/cat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_image = Image.open(\"../data/IMAGES/dog.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vectors are useful because we can then do basic similarity checks using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vec = img2vec.get_vec(cat_image.convert('RGB'), tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_vec = img2vec.get_vec(dog_image.convert('RGB'), tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_vec = img2vec.get_vec(image, tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cos(cat_vec,\n",
    "              dog_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cos(flower_vec,\n",
    "              dog_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense - the dog and cat are more similar than the dog and flower. We now have a way to compare between images outside of the RGB distribution.\n",
    "\n",
    "This notebook is only a very brief introduction to audio and visual deep learning. This is a very important and exciting area of research, especially for social scientists who can make use of state-of-the-art Python tools. More tutorials like this are available in the [Thinking with Deep Learning course notebooks](https://github.com/UChicago-Thinking-Deep-Learning-Course/Tutorials-Homework-Notebooks/tree/main/week-6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <font color=\"red\">*Exercise 3*</font>\n",
    "\n",
    "<font color=\"red\">You can either:\n",
    "    \n",
    "<font color=\"red\">a) Construct cells immediately below this that report the results from experiments in which you place each of images taken or retrieved for the last exercise through the online demos for [caffe](http://demo.caffe.berkeleyvision.org) and [places](http://places.csail.mit.edu/demo.html). Paste the image and the output for both object detector and scene classifier below, beside one another. Calculate precision and recall for caffe's ability to detect objects of interest across your images. What do you think about Places' scene categories and their assignments to your images? \n",
    "    \n",
    "<font color=\"red\">b) Implement any one deep learning example using PyTorch and images. What does the pre-trained model see in your images?\n",
    "    \n",
    "<font color=\"red\">c) Use some form of vectorisation of images - (a deep learning one, or an RGB representation, or HSV) and use simlarity measures or clustering to explore your image data.\n",
    "\n",
    "<font color=\"red\">Could you use image classification to enhance your research project and, if so, how? How would multi-modal data sources make your analysis more powerful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Do androids dream of electric sheep?\n",
    "If text and images can both be turned into vectors, why not transform one into the other? We can turn images to text with classification (e.g., ImageNet) or captioning, and we can turn text to images. Last week we discussed [AI Dungeon](https://play.aidungeon.io/main/home) as an example of text generation, and recently they've been adding images to their storytelling. A popular model right now is [Connecting Text and Images (CLIP)](https://openai.com/blog/clip/) by OpenAI.\n",
    "\n",
    "Here are a few images generated with [Deep Daze](https://github.com/lucidrains/deep-daze), which uses CLIP.\n",
    "\n",
    "<img src=\"https://github.com/lucidrains/deep-daze/raw/main/samples/Mist_over_green_hills.jpg\" width=\"256px\"></img>\n",
    "\n",
    "<center>*mist over green hills*</center>\n",
    "\n",
    "<img src=\"https://github.com/lucidrains/deep-daze/raw/main/samples/Life_during_the_plague.jpg\" width=\"256px\"></img>\n",
    "\n",
    "<center>*life during the plague*</center>\n",
    "\n",
    "<img src=\"https://github.com/lucidrains/deep-daze/raw/main/samples/A_man_painting_a_completely_red_image.png\" width=\"256px\"></img>\n",
    "\n",
    "<center>*a man painting a completely red image*</center>\n",
    "\n",
    "With a decent GPU, you can make these images yourself! Wouldn't this make a cool title slide for your final project presentation?\n",
    "\n",
    "The Deep Daze model can be ran by installing a `pip` package and running on the command prompt.\n",
    "\n",
    "```\n",
    "!pip install deep-daze\n",
    "!imagine \"a robotic aristocrat, in the style of Rembrandt\"\n",
    "```\n",
    "\n",
    "However, I recommend instead using one of the Colab notebooks with diffusion models, which are newer, faster, and produce (in my opinion) cooler output. A great one is [Disco Diffusion](https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb), based on [Guided Diffusion](https://github.com/openai/guided-diffusion) made by... big surprise.... OpenAI! To use that notebook (as of March 2022):\n",
    "\n",
    "1. Click `Copy to Drive` and switch to your new copy.\n",
    "2. Scroll down and change the `text_prompt[0]` from \"A beautiful painting...\" to something related to your final project.\n",
    "3. Click `Runtime -> Change runtime type` and select `GPU`. Note that Colab Pro might give you a better GPU.\n",
    "4. Click `Runtime -> Run all`.\n",
    "\n",
    "The rest of the notebook are just a huge range of settings you can play with for desired output. You can decrease or increase the number of iterations or epochs for faster test runs or a high-quality final product. Keep in mind that creating a video rather than images requires a lot more computation.\n",
    "\n",
    "There is another similar notebook [v-diffusion-pytorch](https://colab.research.google.com/drive/1M4QKDt5ExIo3GLml0YKcJM9gfaX9ptfg) that you can experiment with. (From my experimentation, V Diffusion seems to be the fastest, and ithe quality is better than Deep Daze but not as good as Disco Diffusion.)\n",
    "\n",
    "Remember that many AI tools start out looking like parlor tricks before we're able to scale them up and find practical applications, and this field is progressing at a breakneck rate. We're only at [the beginning of the future](https://80000hours.org/articles/future-generations/)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
