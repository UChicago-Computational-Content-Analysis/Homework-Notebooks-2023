{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Classifying Meanings & Documents\n",
    "\n",
    "Recall that Week 1 of this course focused on how to find and extract text data, while weeks 2 through 4 each covered a different type of content representations:\n",
    "\n",
    "2. computational linguistics (e.g., keyword count)\n",
    "3. clustering and topic modeling (e.g., LDA)\n",
    "4. embeddings (e.g., _word2vec_)\n",
    "\n",
    "Now we show how classification, arguably the most common social science task, can be done with different representations. Note that the second representation, *clustering*, allows us to stably partition text data (e.g., documents, turns of conversation) according to all patterns of covariation among available text features. Classification, by contrast, partitions text data according to only those features and their variation that enable us to mimic and extrapolate human annotations.\n",
    "\n",
    "We will show how to use a variety of classification methods, including Logistic regression, K-nearest neighbor, decision trees and random forests, support vector machines and even a simple neural network, the perceptron. We will also demonstrate ensemble techniques that can link several such methods into a single, more accurate, classification pipeline. We will finally learn to use conventions and metrics to evaluate classifier performance on out-of-sample data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "#For ML\n",
    "import sklearn\n",
    "# import sklearn.naive_bayes\n",
    "import sklearn.tree\n",
    "import sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "import sklearn.decomposition\n",
    "\n",
    "import numpy as np #arrays\n",
    "import matplotlib.pyplot as plt #Plots\n",
    "import matplotlib.colors # For nice colours\n",
    "import seaborn #Makes plots look nice, also heatmaps\n",
    "import scipy as sp #for interp\n",
    "\n",
    "import pyanno #On python3 make sure to pip install pyanno3\n",
    "\n",
    "#We need to import these this way due to how pyanno is setup\n",
    "from pyanno.measures import pairwise_matrix, agreement, cohens_kappa, cohens_weighted_kappa, fleiss_kappa, krippendorffs_alpha, pearsons_rho, scotts_pi, spearmans_rho\n",
    "from pyanno.annotations import AnnotationsContainer\n",
    "from pyanno.models import ModelA, ModelBt, ModelB\n",
    "\n",
    "from functools import reduce\n",
    "from itertools import permutations\n",
    "import math\n",
    "\n",
    "#These are from the standard library\n",
    "import collections\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import glob\n",
    "import pandas\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a course on \"computational\" content analysis, but the best computer today is arguably still the human brain. Our aim is to augment human intelligence and scholarship. One of the most important strategies is to have real, live _Homo sapiens_ annotate text, assigning numbers or categories to individual texts. We can computationally compare annotations across humans, and we can use annotations to train computational models.\n",
    "\n",
    "Let's take a quick look at Rzhetsky et al (2009)'s sample dataset, which can be found [here](https://github.com/enthought/uchicago-pyanno/tree/master/data). This data is the result of a content analytic / content extraction study in which Andrey Rzhetsky and colleagues from the National Library of Medicine, published [here](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000391) in [PLOS Computational Biology](http://journals.plos.org/ploscompbiol/), gave eight annotators 10,000 sentence chunks from biomedical text in biomedical abstracts and articles, then asked them, in a loop design schematically illustrated below that provided 3 independent codings for each document. The sampling strategy pursued diversity by drawing from PubMed abstracts (1000) and full-text articles (9000: 20% from abstracts, 10% from introductions, 20% from methods, 25% from results, and 25% from discussions.) The dataset extract here involves respondents codes for sentences in terms of their *Evidence*: {0, 1, 2, 3, -1} where 0 is the complete lack of evidence, 3 is direct evidence present within the sentence, and -1 is didn't respond. (They also crowdsourced and analyzed *polarity*, *certainty*, and *number*). For example, consider the following two abutting sentence chunks: *\"Because null mutations in toxR and toxT abolish CT and TcpA expression in the El Tor biotype and also attenuate virulence...\"* [i.e., average certainty = 0], *\"...it is likely that the ToxR regulon has functional similarities between the two biotypes despite the clear differences in the inducing parameters observed in vitro\"* [i.e., average certainty = 1].\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks/blob/main/data/loopdesign.png?raw=true\" alt=\"https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks/blob/main/data/loopdesign.png?raw=true\" style=\"width:500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember if you are on Google Colab, you need to upload or `git clone` the datafile\n",
    "# and your path will probably be something like \"#content/drive/MyDrive//data/pyAnno/testdata_numerical.txt\"\n",
    "x = np.loadtxt(\"../data/pyAnno/testdata_numerical.txt\")\n",
    "anno = AnnotationsContainer.from_array(x, missing_values=[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interrogate the AnnotationsContainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno.annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno.missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we assume categorical codes...that each code is qualitatively distinct from each other. Two measures are primarily used for this: Scott's $\\pi$, Cohen's $\\kappa$, and Krippendorff's $\\alpha$ which each measure the extent of agreement between two annotators, but take into account the possibility of the agreement occurring by chance in slightly different ways. Any agreement measure begins with the frequency of codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyanno.measures.agreement.labels_frequency(anno.annotations,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the \"confusion matrix\" or matrix of coded agreements between any two coders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pyanno.measures.agreement.confusion_matrix(anno.annotations[:,0], anno.annotations[:,1],4)\n",
    "print(c)\n",
    "ac = seaborn.heatmap(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scott's $\\pi$ is computed as:\n",
    "\n",
    "$\\pi = \\frac{\\text{Pr}(a)-\\text{Pr}(e)}{1-\\text{Pr}(e)}$\n",
    "\n",
    "Where Pr($a$) is relative observed agreement, and Pr($e$) is expected agreement using joint proportions calculated from the confusion matrix or matrix of coded agreements between any two coders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scotts_pi(anno.annotations[:,0], anno.annotations[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization of Scott's $\\pi$ to $n$ coders is Fleiss' $\\kappa$ (Fleiss called it $\\kappa$ because he thought he was generalizing Cohen's $\\kappa$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleiss_kappa(anno.annotations[::])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krippendorff's $\\alpha$ generalizes of Fleiss' $\\kappa$ to $n$ coders and takes into account the fact that annotations here are not categorically different, but ordinal, by adding a weight matrix in which off-diagonal cells contain weights indicating the seriousness of the disagreement between each score. When produced with no arguments, it simply produces an arithmetic distance (e.g., 3-1=2), such that cells one off the diagonal are weighted 1, two off 2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krippendorffs_alpha(anno.annotations[::])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Scott's $\\pi$, Cohen's $\\kappa$ also takes into account the possibility of the agreement occurring by chance, but in the following way:\n",
    "\n",
    "$\\kappa = \\frac{p_o-p_e}{1-p_e}=1-\\frac{1-p_o}{p_e}$\n",
    "\n",
    "where $p_o$ is the relative observed agreement among raters, and $p_e$ is the hypothetical probability of chance agreement, using the observed data to calculate the probabilities of each observer randomly saying each category. If the raters are in complete agreement then $\\kappa = 1$. If there is no agreement among the raters other than what would be expected by chance (as given by $p_e$), $\\kappa ≤ 0 $. Here, Cohen's $\\kappa$ statistic for the first two annotators is computed. This is probably the most common metric of agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohens_kappa(anno.annotations[:,0], anno.annotations[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pairwise_matrix(cohens_kappa, anno.annotations)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seaborn.heatmap(m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this 8 by 3 loop design will be less stable than an 8 choose 3 combinatorial design, because each codes with more others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also assess the average Cohen's $\\kappa$ for all pairs of coders that have coded against one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_metric_average(metric, array):\n",
    "    \"\"\"Calculate the pairwise metric average for the real elements of metric function run on an array of annotations\"\"\"\n",
    "    p = permutations(range(array[0,:].size),2)\n",
    "    m = [metric(array[:,x[0]], array[:,x[1]]) for x in p]\n",
    "    clean_m = [c for c in m if not math.isnan(c)]\n",
    "    return reduce(lambda a, b: a + b, clean_m)/len(clean_m)    \n",
    " \n",
    "pairwise_metric_average(cohens_kappa, anno.annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As recognized with Krippendorff's flexible $\\alpha$, our scores are *not* categorical, but rather ordered and her considered metric. Weighted $\\kappa$ allows you to count disagreements differently and is useful when codes are ordered as they are here. Here a weight matrix is added to the calculation, in which off-diagonal cells contain weights indicating the seriousness of the disagreement between each score. When automatically produced, it simply produces an arithmetic distance (e.g., 3-1=2), such that cells one off the diagonal are weighted 1, two off 2, etc. Here\n",
    "\n",
    "$\\kappa = 1-\\frac{\\sum^k_{i=1}\\sum^k_{j=1}w_{ij}x_{ij}}{\\sum^k_{i=1}\\sum^k_{j=1}w_{ij}m_{ij}}$\n",
    "\n",
    "where $\\kappa$ = $n$ codes and $w_{ij}$,$x_{ij}$, and $m_{ij}$ represent elements in the weight, observed, and expected matrices, respectively. (Obviously, when diagonal cells contain weights of 0 and off-diagonal cells weights of 1, this equals $\\kappa$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohens_weighted_kappa(anno.annotations[:,0], anno.annotations[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or averaged over the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_metric_average(cohens_weighted_kappa,anno.annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if the annontation data can be understood as indicating real values, we can assess not agreement, but rather the correlation of values (Pearson's $\\rho$) or correlation of ranks (Spearman's $\\rho$) for pairs of coders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = pairwise_matrix(pearsons_rho, anno.annotations)\n",
    "m = pairwise_matrix(spearmans_rho, anno.annotations)\n",
    "an = seaborn.heatmap(n)\n",
    "plt.show()\n",
    "am = seaborn.heatmap(m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or averaged over all comparable pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pairwise_metric_average(pearsons_rho,anno.annotations), pairwise_metric_average(spearmans_rho,anno.annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 1*</font>\n",
    "\n",
    "<font color=\"red\">Perform a content annotation survey of some kind in which at least 3 people evaluate and code each piece of content, using Amazon Mechanical Turk as described in the [MTurk slides on Canvas](https://canvas.uchicago.edu/courses/39937/files/6674661?wrap=1), or by hand with friends.  With the resulting data, calculate, visualize and discuss inter-coder agreement or covariation with appropriate metrics. What does this means for the reliability of human assessments regarding content in your domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Simulated Examples for Classification\n",
    "\n",
    "Here we create a sandbox for you to explore different types of classified data and how different statistical classifiers perform on each type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating example data\n",
    "\n",
    "We start by loading one of the \"cartoon\" or simplified data sets and then dividing it into training and testing sets. To maximize our ability to visualize, each dataset involves two classes, colored yellow and blue, arrayed along two dimensions (`x` and `y`). \n",
    "\n",
    "The four data patterns include: \n",
    "+ `random` in which the two classes are randomly distributed across both dimensions\n",
    "+ `andSplit` in which the two classes are linearly split along one of two dimensions (e.g., men like Adidas)\n",
    "+ `xorSplit` in which the two classes are split, oppositely, along each dimension (e.g., old ladies and young men like Nikes)\n",
    "+ `targetSplit` in which one class is nested within the other in two dimensions (e.g., middle aged, middle income people like vintage Mustangs)\n",
    "+ `multiBlobs` in which 5 classes are placed as bivariate Gaussians at random locations\n",
    "\n",
    "`noise` is a variable [0-1] that ranges from no noise in the prescribed pattern [0] to complete noise/randomness [1].\n",
    "\n",
    "Uncomment (remove the # in front of) each dataset, one at a time, and then run the cell and subsequent cells to examine how each machine learning approach captures each pattern.\n",
    "\n",
    "We use the popular `train_test_split()` function from `sklearn`, and the data pattern function source code is copied below for your convenience from `lucem_illud`'s [cartoons.py](https://github.com/UChicago-Computational-Content-Analysis/lucem_illud/blob/main/lucem_illud/cartoons.py) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(numPerCategory = 500):\n",
    "    datDict = {\n",
    "        'vect' : [np.random.rand(2) * 2 - 1 for i in range(2 * numPerCategory)],\n",
    "        'category' : [i % 2 for i in range(2 * numPerCategory)],\n",
    "    }\n",
    "\n",
    "    return pandas.DataFrame(datDict)\n",
    "\n",
    "def andSplit(noise = 0, numPerCategory = 500):\n",
    "    def genPoint(cat):\n",
    "        y = np.random.random_sample() * 2 - 1\n",
    "        if noise >= 0:\n",
    "            x = np.random.random_sample() - cat - (np.random.random_sample() - cat) * noise\n",
    "        else:\n",
    "            x = (1 - noise * np.random.random_sample()) - cat\n",
    "        return np.array([x, y])\n",
    "    datDict = {\n",
    "        'vect' : [genPoint(i % 2) for i in range(2 * numPerCategory)],\n",
    "        'category' : [i % 2 for i in range(2 * numPerCategory)],\n",
    "    }\n",
    "\n",
    "    return pandas.DataFrame(datDict)\n",
    "\n",
    "def xorSplit(noise = 0, numPerCategory = 500):\n",
    "    def genPoint(cat):\n",
    "        if cat == 1:\n",
    "            if np.random.randint(0,2) < 1:\n",
    "                y = np.random.random_sample() - np.random.random_sample() * noise * 2\n",
    "                x = np.random.random_sample() - np.random.random_sample() * noise * 2\n",
    "            else:\n",
    "                y = np.random.random_sample() - 1 - np.random.random_sample() * noise * 2\n",
    "                x = np.random.random_sample() - 1 - np.random.random_sample() * noise * 2\n",
    "        else:\n",
    "            if np.random.randint(0,2) < 1:\n",
    "                y = np.random.random_sample() - 1 - np.random.random_sample() * noise * 2\n",
    "                x = np.random.random_sample() - np.random.random_sample() * noise * 2\n",
    "            else:\n",
    "                y = np.random.random_sample() - np.random.random_sample() * noise * 2\n",
    "                x = np.random.random_sample() - 1 - np.random.random_sample() * noise * 2\n",
    "        return np.array([x, y])\n",
    "\n",
    "    datDict = {\n",
    "        'vect' : [genPoint(i % 2) for i in range(2 * numPerCategory)],\n",
    "        'category' : [i % 2 for i in range(2 * numPerCategory)],\n",
    "    }\n",
    "\n",
    "    return pandas.DataFrame(datDict)\n",
    "\n",
    "def targetSplit(noise = 0, numPerCategory = 500, innerRad = .3):\n",
    "    def genPoint(cat):\n",
    "        if cat == 0:\n",
    "            r = np.random.random_sample() * innerRad + (1 - innerRad) * np.random.random_sample() * noise\n",
    "        else:\n",
    "            r = np.random.random_sample() * (1 - innerRad) + innerRad - innerRad * np.random.random_sample() * noise\n",
    "        eta = 2 * np.pi * np.random.random_sample()\n",
    "        return np.array([r * np.cos(eta), r * np.sin(eta)])\n",
    "\n",
    "    datDict = {\n",
    "        'vect' : [genPoint(i % 2) for i in range(2 * numPerCategory)],\n",
    "        'category' : [i % 2 for i in range(2 * numPerCategory)],\n",
    "    }\n",
    "\n",
    "    return pandas.DataFrame(datDict)\n",
    "\n",
    "def multiBlobs(noise = 0, numPerCategory = 500, centers = 5):\n",
    "    if isinstance(centers, int):\n",
    "        n_samples = numPerCategory * centers\n",
    "    else:\n",
    "        n_samples = numPerCategory * len(centers)\n",
    "    X, y = sklearn.datasets.make_blobs(n_samples=n_samples, centers=centers, cluster_std = (.8 * (noise * 2 + 1)))\n",
    "    datDict = {\n",
    "        'vect' : list(X),\n",
    "        'category' : y,\n",
    "    }\n",
    "    return pandas.DataFrame(datDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = .2\n",
    "\n",
    "#dfTrain, dfTest = sklearn.model_selection.train_test_split(lucem_illud.random(noise), test_size=.2)\n",
    "#dfTrain, dfTest = sklearn.model_selection.train_test_split(lucem_illud.andSplit(noise), test_size=.2)\n",
    "#dfTrain, dfTest = sklearn.model_selection.train_test_split(lucem_illud.xorSplit(noise), test_size=.2)\n",
    "dfTrain, dfTest = sklearn.model_selection.train_test_split(lucem_illud.targetSplit(noise), test_size=.2)\n",
    "#dfTrain, dfTest = sklearn.model_selection.train_test_split(lucem_illud.multiBlobs(noise), test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily visualize the rendered datasets because they are generated in two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what dfTrain, a simulated data that we just made, looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, dfTrain has two columns, vect and category. We can plot this with `plotter()` from [lucem_illud/lucem_illud/cartoons.py](https://github.com/UChicago-Computational-Content-Analysis/lucem_illud/blob/main/lucem_illud/cartoons.py). Here's the source code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(df):\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    pallet = seaborn.color_palette(palette='rainbow', n_colors= len(set(df['category'])))\n",
    "    for i, cat in enumerate(set(df['category'])):\n",
    "        a = np.stack(df[df['category'] == cat]['vect'])\n",
    "        ax.scatter(a[:,0], a[:, 1], c = pallet[i], label = cat)\n",
    "    ax.legend(loc = 'center right', title = 'Categories')\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotter(dfTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Machine Learning algorithm\n",
    "\n",
    "We can now pick a model, there are many more options in `scikit-learn`. These are just a few examples, which array along the machine learning \"tribes\" described in Pedro Domingos _The Master Algorithm_.\n",
    "\n",
    "Uncomment (remove the # in front of) each algorithm one at a time, then run the cell and subsequent cells to evaluate how it learns to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes\n",
    "#clf = sklearn.naive_bayes.GaussianNB()\n",
    "\n",
    "#Analogizes\n",
    "#clf = sklearn.svm.SVC(kernel = 'linear', probability = True) #slow, set probability = False to speed up\n",
    "#clf = sklearn.svm.SVC(kernel = 'poly', degree = 3, probability = True) #slower\n",
    "clf = sklearn.neighbors.KNeighborsClassifier(5, weights='distance')# k, 'distance' or 'uniform'\n",
    "\n",
    "#Classical Regression\n",
    "#clf = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "#Symbolists\n",
    "#clf = sklearn.tree.DecisionTreeClassifier()\n",
    "#clf = sklearn.ensemble.RandomForestClassifier()\n",
    "\n",
    "#Connectionists\n",
    "#clf = sklearn.neural_network.MLPClassifier()\n",
    "\n",
    "#Ensemble\n",
    "#clf = sklearn.ensemble.GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using sklearn here. What is sklearn? It's an open source machine learning library for Python. We use sklearn because it features various machine learning algorithms and it works well with NumPy library. You don't need to understand the sklearn package in detail for the moment, but let's see what methods and attributes sklearn has, because we're going to use some of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the model by giving it our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(np.stack(dfTrain['vect'], axis=0), dfTrain['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \"stack\" function is used. Why did we use stack function here? This is because dfTrain['vect'] is a sequence, while clf.fit() takes an array element. Let's see what dfTrain['vect'] looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain['vect']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see what np.stack(dfTrain['vect']) does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(dfTrain['vect'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, stack function takes a sequence of arrays (which have the same shape) and joins them along a new axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm evaluation\n",
    "\n",
    "We can look at few measurements of each classifier's performance by using the testing set. These `lucem_illud` functions are in [metrics.py](https://github.com/UChicago-Computational-Content-Analysis/lucem_illud/blob/main/lucem_illud/metrics.py), and we will skip copying source code this time due to length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.evaluateClassifier(clf, dfTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lets us look at which classes do better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotConfusionMatrix(clf, dfTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greater the area under the curve the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotMultiROC(clf, dfTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the regions the classifer identifies as one class or the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotregions(clf, dfTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we do the same for real data\n",
    "\n",
    "Available data sets include:\n",
    "+ Reddit threads \"classified\" by thread topic\n",
    "+ 20 newsgroups \"classified\" by group topic\n",
    "+ Senate press releases \"classified\" by Senator (2 senators)\n",
    "+ Senate press releases \"classified\" by Senator (5 senators)\n",
    "+ Emails classified as Spam or Ham\n",
    "\n",
    "Note that if you're on Google Colab and have not `git clone`ed the [class repo](https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks), you can set a `dataDirectory=` argument to match the path on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfTrain, dfTest = sklearn.model_selection.train_test_split(lucem_illud.loadReddit(dataDirectory='../data/'), test_size=.2)\n",
    "dfTrain, dfTest = sklearn.model_selection.train_test_split(lucem_illud.loadReddit(), test_size=.2)\n",
    "#dfTrain, dfTest = sklearn.model_selection.train_test_split(lucem_illud.loadNewsGroups(), test_size=.2)\n",
    "#dfTrain, dfTest = sklearn.model_selection.train_test_split(lucem_illud.loadSenateSmall(), test_size=.2)\n",
    "#dfTrain, dfTest = sklearn.model_selection.train_test_split(lucem_illud.loadSenateLarge(), test_size=.2)\n",
    "#dfTrain, dfTest = sklearn.model_selection.train_test_split(lucem_illud.loadSpam(), test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'../data/reddit.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classical Regression\n",
    "clf = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "#Bayes\n",
    "# clf = sklearn.naive_bayes.GaussianNB()\n",
    "\n",
    "#Analogizes\n",
    "#clf = sklearn.svm.SVC(kernel = 'linear', probability = True) #slow, set probability = False to speed up, but lose ROC\n",
    "#clf = sklearn.svm.SVC(kernel = 'poly', degree = 3, probability = True) #slower\n",
    "#clf = sklearn.neighbors.KNeighborsClassifier(5, weights='distance')# k, 'distance' or 'uniform'\n",
    "\n",
    "#Symbolists\n",
    "#clf = sklearn.tree.DecisionTreeClassifier()\n",
    "#clf = sklearn.ensemble.RandomForestClassifier()\n",
    "\n",
    "#Connectionists\n",
    "#clf = sklearn.neural_network.MLPClassifier()\n",
    "\n",
    "#Ensemble\n",
    "#clf = sklearn.ensemble.GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(np.stack(dfTrain['vect'], axis=0), dfTrain['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.evaluateClassifier(clf, dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotConfusionMatrix(clf, dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotMultiROC(clf, dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotregions(clf, dfTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 2*</font>\n",
    "\n",
    "<font color=\"red\">Go back through all of the cells above and generate 10 distinct artificial datasets and classify them with all of the available methods. Add a cell immediately below and describe which classifier(s) worked best with which artificially constructed data source and why. Then go through all of the empirical datasets (i.e., Newsgroups, Senate Small, Senate Large, Email Spam) and classify them with all available methods. Add a second cell immediately below and describe which classifier(s) worked best with which data set and why.\n",
    "\n",
    "<font color=\"red\">***Stretch*** (but also required) Wander through the SKLearn documentation available [here](http://scikit-learn.org/stable/), particularly perusing the classifiers. In cells following, identify and implement a new classifier that we have not yet used (e.g., AdaBoost, CART) on one artificial dataset and one real dataset (used above). Then, in the next cell describe the classifier, detail how it compares with the approaches above, and why it performed better or worse than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinton / Obama Press Releases\n",
    "\n",
    "We often will not have nicely prepared data, so we will work though the proccess of cleaning and structuring in more detail here:\n",
    "\n",
    "While the Clinton and Obama Senatorial Press Releases are not hand-coded, we can imagine that we have been given a stack of such press releases, but lost the metadata associated with which senatorial office issued which. If we label a few of them, how well can our classifier do at recovering the rest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObamaClintonReleases = pandas.read_csv('../data/ObamaClintonReleases.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm pretty sure that you're all familiar with pandas, but, just to clarify: why do we use pandas here? pandas is a Python library which is widely used for analyzing and wrangling data. In particular, pandas loads data and creates data frame, a Python object that looks familiar to us (since it looks like a excel table) and easy to work with. So, using pandas.read_csv function, we take in the csv file and convert it into a data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ObamaClintonReleases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a DataFrame! and it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObamaClintonReleases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! Let's turn the 'targetSenator' column into a binary category variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObamaClintonReleases['category'] = [s == 'Obama' for s in ObamaClintonReleases['targetSenator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObamaClintonReleases['tokenized_text'] = ObamaClintonReleases['text'].apply(lambda x: lucem_illud.word_tokenize(x))\n",
    "ObamaClintonReleases['normalized_text'] = ObamaClintonReleases['tokenized_text'].apply(lambda x: lucem_illud.normalizeTokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training data and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdBackFraction = .2\n",
    "train_data_df, test_data_df = sklearn.model_selection.train_test_split(ObamaClintonReleases, test_size=holdBackFraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data_df))\n",
    "print(len(test_data_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try with a logistic regression, which may be familiar to you from statistical methods classes. First, we must turn the training dataset into a tf-idf matrix (`lucem_illud.generateVecs()` will help with this but for now we are doing it the long way):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=100, min_df=2, stop_words='english', norm='l2')\n",
    "TFVects = TFVectorizer.fit_transform(train_data_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core function here is TfidfVectorizer, which takes a collection of raw documents and turn them to a tf-idf matrix. Just to recap: tf-idf means term frequency-inverse document frequency, a statistic (or, more precisely, a product of two statistics, term frequency and inverse document frequency) that shows the importance of a term vis-a-vis documents. TF, or, term frequency, counts how many times a term is used in a document; IDF, or, inverse-document-frequency, measures common or rare a term appears across documents. \n",
    "\n",
    "Let's look at three parameters of TfidfVectorizer: max_df = 100, min_df = 2, and norm='l2'. What do those parameters mean? \n",
    "\n",
    "(1) max_df = 100\n",
    "\n",
    "Here, we specified a thredhold of 100, and the terms that have a document frequency higher than 100 would be ignored.\n",
    "\n",
    "(2) min_df = 2\n",
    "\n",
    "We specified a lower bound, 2, and the terms that have a document frequency lower than 2 will be ignored.\n",
    "\n",
    "(3) norm = 'l2'\n",
    "\n",
    "This parameter is about vector normalization. In machine learning, we commonly normalize vectors, i.e., change the length of vectors and turn them into a unit vector, before passing them into algorithms. There are various ways of normalizations, and this parameter specifies how we normalize vectors. Here, we set the norm to l2, in which case, we normalize the vectors such that squares of vector elements sums to 1. Alternatively, we can set it to l1, in which case the sum of absolute values of vector elements, not the square of vector elements, is 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_transform() literally fits to data and then transform it. So, fit_transform() is just a combination of two steps--(1) fitting parameters to data; (2) then, using the vocabulary and document frequencies learned by fit(), transforming documents into document-term matrix. So, it's the same as fit followed by transform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use the CountVectorizer instead, which simply produces a matrix of word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TFVects.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save this in the dataframe to make things easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['vect'] = [np.array(v).flatten() for v in TFVects.todense()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks simple, but we need to know what todense() does here. todense() returns a dense matrix representation of the matrix. Why do we need this? As you can see above, the TFVects, a document-term matrix, has 11349 columns, and this matrix is sparse, in the sense that it is comprised mostly of zeros. Dense matrices, in contrast, are the matrices that are comprised of mostly non-zeros. Then why do we make sparse matrices into dense ones? Because zero values don't contain important information but take up so much memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a regression, we cannot have more variables than cases. So, we need to first do a dimension reduction. First, we will approah this with PCA. You have previously seen this in week 3. Here we are not concerned about visualization, but rather classification and so all principal components are calculated. Watch out: we have to use `stack` not `sum` for combining the vectors. We note that you could also use topic loading and embedding dimensions as featured variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA()\n",
    "reduced_data = pca.fit_transform(np.stack(train_data_df['vect'], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store the PCA space vectors in the dataframe too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['pca'] = [r for r in reduced_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization in 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.axis('off')\n",
    "pallet = seaborn.color_palette(palette='coolwarm', n_colors = 2)\n",
    "\n",
    "#Plot Obama\n",
    "a = np.stack(train_data_df[train_data_df['category']]['pca'])\n",
    "ax.scatter(a[:,0], a[:, 1], c = pallet[0], label = \"True\")\n",
    "\n",
    "#Plot not Obama\n",
    "a = np.stack(train_data_df[train_data_df['category'].eq(False)]['pca'])\n",
    "ax.scatter(a[:,0], a[:, 1], c = pallet[1], label = \"False\")\n",
    "    \n",
    "ax.legend(loc = 'upper right', title = 'Is Obama')\n",
    "plt.title('True Classes, Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA cannot distinguish Obama very well. Let's perform a screeplot to see how many Principal Components we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(train_data_df)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize = (16, 5), sharey=True)\n",
    "\n",
    "eigen_vals = np.arange(n) + 1\n",
    "ax1.plot(eigen_vals, pca.explained_variance_ratio_, 'ro-', linewidth=1)\n",
    "ax1.set_title('Scree Plot (Full)')\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Proportion of Explained Variance')\n",
    "\n",
    "eigen_vals = np.arange(50) + 1\n",
    "ax2.plot(eigen_vals, pca.explained_variance_ratio_[:50], 'ro-', linewidth=1)\n",
    "ax2.set_title('Scree Plot (First 50 Principal Components)')\n",
    "ax2.set_xlabel('Principal Component')\n",
    "ax2.set_ylabel('Proportion of Explained Variance')\n",
    "\n",
    "\n",
    "eigen_vals = np.arange(20) + 1\n",
    "ax3.plot(eigen_vals, pca.explained_variance_ratio_[:20], 'ro-', linewidth=2)\n",
    "ax3.set_title('Scree Plot (First 50 Principal Components)')\n",
    "ax3.set_xlabel('Principal Component')\n",
    "ax3.set_ylabel('Proportion of Explained Variance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose the first 10 pricipal components as our covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['pca_reduced_10'] = train_data_df['pca'].apply(lambda x: x[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit a logistic regression to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression()\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_10'], axis=0), train_data_df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the logistic regression performs on the training dataset from which we develop the model. Unfortunately, the mean accuracy is only about 64%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.score(np.stack(train_data_df['pca_reduced_10'], axis=0), train_data_df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it perform on the testing dataset, which we \"held out\" and did not use for model training? We need to repeat all the steps on the testing data, but without retraining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create vectors\n",
    "TFVects_test = TFVectorizer.transform(test_data_df['text'])\n",
    "test_data_df['vect'] = [np.array(v).flatten() for v in TFVects_test.todense()]\n",
    "\n",
    "#PCA\n",
    "reduced_data_test = pca.transform(np.stack(test_data_df['vect'], axis=0))\n",
    "test_data_df['pca'] = [r for r in reduced_data_test]\n",
    "test_data_df['pca_reduced_10'] = test_data_df['pca'].apply(lambda x: x[:10])\n",
    "\n",
    "#Test\n",
    "logistic.score(np.stack(test_data_df['pca_reduced_10'], axis=0), test_data_df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly poorer. How about using more dimensions (40)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['pca_reduced_40'] = train_data_df['pca'].apply(lambda x: x[:40])\n",
    "test_data_df['pca_reduced_40'] = test_data_df['pca'].apply(lambda x: x[:40])\n",
    "\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_40'], axis=0), train_data_df['category'])\n",
    "\n",
    "print(\"Training:\")\n",
    "print(logistic.score(np.stack(train_data_df['pca_reduced_40'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(logistic.score(np.stack(test_data_df['pca_reduced_40'], axis=0), test_data_df['category']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or still more (100)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['pca_reduced_100'] = train_data_df['pca'].apply(lambda x: x[:100])\n",
    "test_data_df['pca_reduced_100'] = test_data_df['pca'].apply(lambda x: x[:100])\n",
    "\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_100'], axis=0), train_data_df['category'])\n",
    "\n",
    "print(\"Training:\")\n",
    "print(logistic.score(np.stack(train_data_df['pca_reduced_100'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(logistic.score(np.stack(test_data_df['pca_reduced_100'], axis=0), test_data_df['category']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even more (200)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['pca_reduced_200'] = train_data_df['pca'].apply(lambda x: x[:200])\n",
    "test_data_df['pca_reduced_200'] = test_data_df['pca'].apply(lambda x: x[:200])\n",
    "\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_200'], axis=0), train_data_df['category'])\n",
    "\n",
    "print(\"Training:\")\n",
    "print(logistic.score(np.stack(train_data_df['pca_reduced_200'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(logistic.score(np.stack(test_data_df['pca_reduced_200'], axis=0), test_data_df['category']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is becoming ridiculous (400)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['pca_reduced_400'] = train_data_df['pca'].apply(lambda x: x[:400])\n",
    "test_data_df['pca_reduced_400'] = test_data_df['pca'].apply(lambda x: x[:400])\n",
    "\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_400'], axis=0), train_data_df['category'])\n",
    "\n",
    "print(\"Training:\")\n",
    "print(logistic.score(np.stack(train_data_df['pca_reduced_400'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(logistic.score(np.stack(test_data_df['pca_reduced_400'], axis=0), test_data_df['category']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of covariates would overfit our data, and it seems that using a logistic regression, our prediction accuracy is at best about 65%. We can, however, try a logistic regression that uses the TF-IDF scores for each word, but with an L1 regularization or L1-norm loss function, which is also known as least absolute deviations (LAD), least absolute errors (LAE) or L1 penalty. It minimizes the sum of the absolute differences (S) between the target value ($Y_i$) and the estimated values ($f(x_i)$) and prunes all insignificant variables (i.e., word TF-IDF scores):\n",
    "\n",
    "$S=\\sum^n_{i=1}|y_i=f(x_i)|$\n",
    "\n",
    "The result is a model retaining only the most individually significant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_l1= sklearn.linear_model.LogisticRegression(penalty='l2')\n",
    "logistic_l1.fit(np.stack(train_data_df['vect'], axis=0), train_data_df['category'])\n",
    "print(logistic_l1.score(np.stack(train_data_df['vect'], axis=0), train_data_df['category']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using training data, and then test it on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logistic_l1.score(np.stack(test_data_df['vect'], axis=0), test_data_df['category']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "81% accuracy seems like the best we can get by using a logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 3*</font>\n",
    "\n",
    "<font color=\"red\">In the cells immediately following, perform logistic regression classification using training, testing and uncoded (i.e., data you didn't code by hand but want to use your model on) data from texts and hand-classifications associated with your final project (e.g., these could be crowd-sourced codes gathered through Amazon Mechanical Turk in Exercise 1). Visualize the confusion matrix for training and testing sets. Calculate precision, recall, the F-measure, and AUC, then perform an ROC visualization. How do these classifiers perform? Exrapolate codes from these models to all uncoded data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees can be used to predict both categorical/class labels (i.e., classification) and continuous labels (i.e., regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_df = lucem_illud.multiBlobs(noise=.2, centers=[(0,0), (0,5), (5,0), (-5,0), (0,-5)])\n",
    "df_exampleTree_train, df_exampleTree_test = sklearn.model_selection.train_test_split(blobs_df, test_size=.2)\n",
    "lucem_illud.plotter(df_exampleTree_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import our Decision Tree classifier from sklearn.tree (familiar syntax) and fit it using the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree = sklearn.tree.DecisionTreeClassifier(max_depth=4,random_state=0)\n",
    "clf_tree.fit(np.stack(df_exampleTree_train['vect'], axis =0), df_exampleTree_train['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what's going on visually with the classification: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotregions(clf_tree, df_exampleTree_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.evaluateClassifier(clf_tree, df_exampleTree_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(df_exampleTree_test['category'],clf_tree.predict(np.stack(df_exampleTree_test['vect'], axis = 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we trim the tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depthvec = []\n",
    "scorevec = []\n",
    "for i in range(1,20):\n",
    "    tree2 = sklearn.tree.DecisionTreeClassifier(max_depth=i,random_state=0)\n",
    "    tree2.fit(np.stack(df_exampleTree_train['vect'], axis =0), df_exampleTree_train['category'])\n",
    "    score = sklearn.metrics.accuracy_score(df_exampleTree_test['category'], tree2.predict(np.stack(df_exampleTree_test['vect'], axis = 0)))\n",
    "    depthvec.append(i)\n",
    "    scorevec.append(score)\n",
    "plt.scatter(depthvec,scorevec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select different layers of the decision tree or \"prune\" it. At approximately four layers down in the decision tree, the shape is somewhat odd, suggesting that our model is overfitting beyond those four layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining multiple overfitting estimators turns out to be a key idea in machine learning. This is called **bagging** and is a type of **ensemble** method. The idea is to make many randomized estimators--each can overfit, as decision trees are wont to do--but then to combine them, ultimately producing a better classification. A **random forest** is produced by bagging decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = sklearn.tree.DecisionTreeClassifier(max_depth=10) #Create an instance of our decision tree classifier.\n",
    "\n",
    "bag = sklearn.ensemble.BaggingClassifier(tree, n_estimators=100, max_samples=0.8, random_state=1) #Each tree uses up to 80% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.fit(np.stack(df_exampleTree_train['vect'], axis =0), df_exampleTree_train['category']) #Fit the bagged classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotregions(bag, df_exampleTree_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.evaluateClassifier(bag, df_exampleTree_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotConfusionMatrix(bag, df_exampleTree_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 4*</font>\n",
    "\n",
    "<font color=\"red\">In the cells immediately following, perform decision tree and random forest classification (binary, multinomial or continuous) using training, testing and extrapolation (uncoded) data from texts and hand-classifications associated with your final project. As with ***Exercise 2***, these could be crowd-sourced codes gathered through Amazon Mechanical Turk last week. Visualize the classification of data points. Calculate relevant metrics (e.g., precision, recall, the F-measure, and AUC). Now build an ensemble classifier by bagging trees into a random forest. Visualize the result. How do these classifiers perform? What does ensemble learning do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Nearest neighbors classifier takes a simpler premise than those before: Find the closest labeled datapoint in set and \"borrow\" its label.\n",
    "\n",
    "Let's use newsgroup data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = sklearn.datasets.fetch_20newsgroups(data_home = '../data') #Free data to play with: documents from a newsgroup corpus.\n",
    "newsgroups.target_names #Possible categories, i.e., the newsgroups\n",
    "\n",
    "target_categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics'] #Can change these of course\n",
    "\n",
    "newsgroupsDF = pandas.DataFrame(columns = ['text', 'category', 'source_file'])\n",
    "for category in target_categories:\n",
    "    print(\"Loading data for: {}\".format(category))\n",
    "    ng = sklearn.datasets.fetch_20newsgroups(categories = [category], remove=['headers', 'footers', 'quotes'], data_home = '../data')\n",
    "    newsgroupsDF = newsgroupsDF.append(pandas.DataFrame({'text' : ng.data, 'category' : [category] * len(ng.data), 'source_file' : ng.filenames}), ignore_index=True)\n",
    "\n",
    "newsgroupsDF['tokenized_text'] = newsgroupsDF['text'].apply(lambda x: lucem_illud.word_tokenize(x))\n",
    "newsgroupsDF['normalized_text'] = newsgroupsDF['tokenized_text'].apply(lambda x: lucem_illud.normalizeTokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroupsDF[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a testing and training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdBackFraction = .2\n",
    "train_ng_df, test_ng_df = sklearn.model_selection.train_test_split(newsgroupsDF, test_size=holdBackFraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our k-nearest neighbors classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 15\n",
    "weights=\"uniform\"\n",
    "clf_knearest = sklearn.neighbors.KNeighborsClassifier(n_neighbors, weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to classify using the TF-IDF vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFVectorizer_ng = sklearn.feature_extraction.text.TfidfVectorizer(max_df=100, min_df=2, stop_words='english', norm='l2')\n",
    "TFVects_ng = TFVectorizer_ng.fit_transform(train_ng_df['text'])\n",
    "train_ng_df['vect'] = [np.array(v).flatten() for v in TFVects_ng.todense()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knearest.fit(np.stack(train_ng_df['vect'], axis = 0), train_ng_df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.evaluateClassifier(clf_knearest, train_ng_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets look at the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create vectors\n",
    "TFVects_test = TFVectorizer_ng.transform(test_ng_df['text'])\n",
    "test_ng_df['vect'] = [np.array(v).flatten() for v in TFVects_test.todense()]\n",
    "\n",
    "#Add to df\n",
    "test_ng_df['nb_predict'] = clf_knearest.predict(np.stack(test_ng_df['vect'], axis=0))\n",
    "\n",
    "#Test\n",
    "print(\"Testing score:\")\n",
    "print(clf_knearest.score(np.stack(test_ng_df['vect'], axis=0), test_ng_df['category']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's produce another confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotConfusionMatrix(clf_knearest, test_ng_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can produce the PCA space visual if you want, altough it can take a very long time, so we'll leave it optionally commented out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lucem_illud.plotregions(clf_knearest, test_ng_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 5*</font>\n",
    "\n",
    "<font color=\"red\">In the cells immediately following, perform and visualize k-nearest neighbor classification using training, testing and extrapolation (uncoded) data from texts and hand-classifications associated with your final project. Visualize the classification of data points and calculate relevant metrics (e.g., precision, recall, the F-measure, and AUC). Articulate how the *k*-nearest neighbor approach relates to *k*-means clustering explored in ***week 3***?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SVMs\n",
    "\n",
    "Now we will examine Support Vector Machines, an approach that creates the partition that preserves the \"maximum margin\" between classes.\n",
    "\n",
    "We will use a few sub forums from reddit--which tend to share text rather than memes--namely `talesfromtechsupport`, `badroommates`, `weeabootales` and `relationships`. The top 100 text posts from each have been saved to `data/reddit.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditDf = pandas.read_csv('../data/reddit.csv', index_col = 0)\n",
    "\n",
    "#Drop a couple missing values\n",
    "\n",
    "redditDf = redditDf.dropna()\n",
    "\n",
    "#Set category\n",
    "\n",
    "redditDf['category'] = redditDf['subreddit']\n",
    "\n",
    "#tokenize and normalize\n",
    "redditDf['tokenized_text'] = redditDf['text'].apply(lambda x: lucem_illud.word_tokenize(x))\n",
    "redditDf['normalized_text'] = redditDf['tokenized_text'].apply(lambda x: lucem_illud.normalizeTokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tf.idf the data to make our vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditTFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=0.5, min_df=3, stop_words='english', norm='l2')\n",
    "redditTFVects = redditTFVectorizer.fit_transform([' '.join(l) for l in redditDf['normalized_text']])\n",
    "redditDf['vect'] = [np.array(v).flatten() for v in redditTFVects.todense()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize the model and make a train test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdBackFraction = .2\n",
    "train_redditDf, test_redditDf = sklearn.model_selection.train_test_split(redditDf, test_size=holdBackFraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = sklearn.svm.SVC(kernel='linear', probability = False)\n",
    "#probability = True is slower but  lets you call predict_proba()\n",
    "clf_svm.fit(np.stack(train_redditDf['vect'], axis=0), train_redditDf['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and consider the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.evaluateClassifier(clf_svm, test_redditDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotConfusionMatrix(clf_svm, test_redditDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotregions(clf_svm, test_redditDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets\n",
    "\n",
    "We include an example of a simple neural network, the Multi-layer Perceptron (MLP) that learns a function $f(\\cdot): R^m \\rightarrow R^o$ by training on a dataset, where $m$ is the number of dimensions for input and $o$ is the number of dimensions for output. Given a set of features $X = {x_1, x_2, ..., x_m}$ and a target $y$, it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. The following figure shows a one hidden layer MLP with scalar output. ![title](../data/multilayerperceptron_network.png) The leftmost layer, known as the input layer, consists of a set of \"neurons\" $\\{x_i | x_1, x_2, ..., x_m\\}$ representing the input features (e.g., weighted words). Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation $w_1x_1 + w_2x_2 + ... + w_mx_m$, followed by a non-linear activation function $g(\\cdot):R \\rightarrow R$ - like the logistic or hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nn = sklearn.neural_network.MLPClassifier()\n",
    "clf_nn.fit(np.stack(train_redditDf['vect'], axis=0), train_redditDf['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.evaluateClassifier(clf_nn, test_redditDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotConfusionMatrix(clf_nn, test_redditDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucem_illud.plotregions(clf_nn, test_redditDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 6*</font>\n",
    "\n",
    "<font color=\"red\">In the cells immediately following, perform a neural network classification and calculate relevant metrics (e.g., precision, recall, the F-measure, and AUC). How does this classify relevant to *k*-nearest neighbor, logistic and decision-tree approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cJ-SkJpAdhp"
   },
   "source": [
    "## Classification with BERT Pipelines\n",
    "Now, instead of classifying based on \"old\" NLP methods, we will now use the popular Transformers package default 'pipelines'. This package has methods for a wide range of common NLP tasks using contextual word embeddings. Note that for this homework, we are not building new contextual models ourselves, merely using existing models in Transformers. We will spend Weeks 7 and 8 learning more about Transformers, so do not worry about fully understanding them this week.\n",
    "\n",
    "[Transformers Documentation](https://huggingface.co/transformers/)\n",
    "\n",
    "[Transformers GitHub](https://github.com/huggingface/transformers)\n",
    "\n",
    "The following sections of code are taken from the [Summary of Tasks](https://huggingface.co/transformers/task_summary.html) page in the Transformers documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqTAhBf3-ZMi"
   },
   "source": [
    "### Sequence Classification (e.g., sentiment)\n",
    "Sequence classification is the task of classifying sequences according to a given number of classes. An example of\n",
    "sequence classification is the GLUE dataset. If you would like to fine-tune a\n",
    "model on a GLUE sequence classification task, you may leverage the [run_glue.py](https://github.com/huggingface/transformers/tree/master/examples/text-classification/run_glue.py) and\n",
    "[run_pl_glue.py](https://github.com/huggingface/transformers/tree/master/examples/text-classification/run_pl_glue.py) or\n",
    "[run_tf_glue.py](https://github.com/huggingface/transformers/tree/master/examples/text-classification/run_tf_glue.py) scripts.\n",
    "\n",
    "Here is an example of using pipelines to do sentiment analysis: identifying if a sentence is positive or negative. It\n",
    "leverages a fine-tuned model on sst2, which is a GLUE task.\n",
    "\n",
    "This returns a label (\"POSITIVE\" or \"NEGATIVE\") alongside a score, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5eXH5lA-ZMi",
    "outputId": "d0abdeb0-c986-4e08-a1f3-df27146fe8f8"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "result = nlp(\"I hate you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
    "result = nlp(\"I love you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dYX9haN-ZMj"
   },
   "source": [
    "Here is an example of doing a sequence classification using a model to determine if two sequences are paraphrases of\n",
    "each other. The process is the following:\n",
    "\n",
    "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it\n",
    "   with the weights stored in the checkpoint.\n",
    "2. Build a sequence from the two sentences, with the correct model-specific separators token type ids and attention\n",
    "   masks (`PreTrainedTokenizer.encode` and `PreTrainedTokenizer.__call__` take\n",
    "   care of this).\n",
    "3. Pass this sequence through the model so that it is classified in one of the two available classes: 0 (not a\n",
    "   paraphrase) and 1 (is a paraphrase).\n",
    "4. Compute the softmax of the result to get probabilities over the classes.\n",
    "5. Print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmUMMNsM-ZMk",
    "outputId": "02b1a7d3-11d0-43f7-dfbc-4da417f20036"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2oChnp1-ZMm"
   },
   "source": [
    "### Extractive Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12uRPuEq-ZMm"
   },
   "source": [
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a\n",
    "model on a SQuAD task, you may leverage the [run_squad.py](https://github.com/huggingface/transformers/tree/master/examples/question-answering/run_squad.py) and\n",
    "[run_tf_squad.py](https://github.com/huggingface/transformers/tree/master/examples/question-answering/run_tf_squad.py) scripts.\n",
    "\n",
    "\n",
    "Here is an example of using pipelines to do question answering: extracting an answer from a text given a question. It\n",
    "leverages a fine-tuned model on SQuAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ast1vLPT-ZMm"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"question-answering\")\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5fq8iB0-ZMn"
   },
   "source": [
    "This returns an answer extracted from the text, a confidence score, alongside \"start\" and \"end\" values, which are the\n",
    "positions of the extracted answer in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMSCEtkU-ZMn",
    "outputId": "9d33658c-34b5-4afc-fd85-61723551eef4"
   },
   "outputs": [],
   "source": [
    "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "result = nlp(question=\"What is a good example of a question answering dataset?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4t3RxO8D-ZMn"
   },
   "source": [
    "Here is an example of question answering using a model and a tokenizer. The process is the following:\n",
    "\n",
    "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it\n",
    "   with the weights stored in the checkpoint.\n",
    "2. Define a text and a few questions.\n",
    "3. Iterate over the questions and build a sequence from the text and the current question, with the correct\n",
    "   model-specific separators token type ids and attention masks.\n",
    "4. Pass this sequence through the model. This outputs a range of scores across the entire sequence of tokens (question and\n",
    "   text), for both the start and end positions.\n",
    "5. Compute the softmax of the result to get probabilities over the tokens.\n",
    "6. Fetch the tokens from the identified start and stop values, convert those tokens to a string.\n",
    "7. Print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbvf9egg-ZMo",
    "outputId": "90a0c3c9-fe5f-4f63-8d4b-efa360ac7575"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "text = r\"\"\"\n",
    "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "questions = [\n",
    "    \"How many pretrained models are available in 🤗 Transformers?\",\n",
    "    \"What does 🤗 Transformers provide?\",\n",
    "    \"🤗 Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 7*</font>\n",
    "\n",
    "<font color=\"red\">In the cells immediately following, use the pipeline functions or the word or sentence vector functions (e.g., similarity) to explore the social game underlying the production and meaning of texts associated with your final project. How does BERT help you gain insight regarding your research question that is similar and different from prior methods?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
